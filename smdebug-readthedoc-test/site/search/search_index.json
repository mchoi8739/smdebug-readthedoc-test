{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon SageMaker Debugger Table of Contents Overview SageMaker Debugger in action Install How It Works Examples Further Documentation Release Notes Overview Amazon SageMaker Debugger automates the debugging process of machine learning training jobs. From training jobs, Debugger allows you to run your own training script (Zero Script Change experience) using Debugger built-in features\u2014 Hook and Rule \u2014to capture tensors, have flexibility to build customized Hooks and Rules for configuring tensors as you want, and make the tensors available for analysis by saving in an Amazon S3 bucket, all through a flexible and powerful API. The smdebug library powers Debugger by calling the saved tensors from the S3 bucket during the training job. smdebug retrieves and filters the tensors generated from Debugger such as gradients, weights, and biases. Debugger helps you develop better, faster, and cheaper models by minimally modifying estimator, tracing the tensors, catching anomalies while training models, and iterative model pruning. Debugger supports TensorFlow, PyTorch, MXNet, and XGBoost frameworks. The following list is a summary of the main functionalities of Debugger: Zero Script Change experience on SageMaker when using supported containers Full visibility into any tensor part of the training process Real-time training job monitoring through Rules Automated anomaly detection and state assertions through built-in and custom Rules on SageMaker Actions on your training jobs based on the status of Rules Interactive exploration of saved tensors Distributed training support TensorBoard support See How it works for more details. SageMaker Debugger in Action Through the model pruning process using Debugger and smdebug , you can iteratively identify the importance of weights and cut neurons below a threshold you define. This process allows you to train the model with significantly fewer neurons, which means a lighter, more efficient, faster, and cheaper model without compromising accuracy. See Using SageMaker Debugger and SageMaker Experiments for iterative model pruning notebook for visualization and further information. Use Debugger with XGBoost in SageMaker Studio to save feature importance values and plot them in a notebook during training. Use Debugger with TensorFlow in SageMaker Studio to run built-in rules and visualize the loss. Install SageMaker Debugger smdebug library runs on Python 3.x. Install smdebug through: pip install smdebug Debugger Usage and Supported Frameworks There are two ways in which you can enable SageMaker Debugger while training on SageMaker\u2014Zero Script Change and Bring Your Own Training Container (BYOC). Zero Script Change You can use your own training script while using AWS Deep Learning Containers (DLC) in TensorFlow, PyTorch, MXNet, and XGBoost frameworks. The AWS DLCs enable you to use Debugger with no changes to your training script by automatically adding SageMaker Debugger's Hook . The following table shows currently supported versions of the four frameworks for Zero Script Change experience. Framework Version TensorFlow 1.15, 1.15.2, 2.1 MXNet 1.6 PyTorch 1.3, 1.4 XGBoost >=0.90-2 As Built-in algorithm For the full list and information of the AWS DLCs, see Deep Learning Containers Images . Bring Your Own Training Container smdebug supports frameworks other than the ones listed in the previous Zero Script Change section. You can use your own training script by adding a minimal modification. Currently supported versions of frameworks are listed in the following table. Framework Versions TensorFlow 1.14. 1.15, 2.0.1, 2.1.0 Keras (with TensorFlow backend) 2.3 MXNet 1.4, 1.5, 1.6 PyTorch 1.2, 1.3, 1.4 XGBoost As Framework Support for Distributed Training and Known Limitations Distributed Training Horovod Supported TF 1.15, PT 1.4, MX 1.6 Not supported TF 2.x, PT 1.5 Parameter Server-based Not supported How It Works Amazon SageMaker Debugger uses the construct of a Hook to save the values of requested tensors throughout the training process. You can then setup a Rule job which simultaneously monitors and validates these tensors to ensure that training is progressing as expected. A Rule checks for vanishing gradients, exploding tensor values, or poor weight initialization. Rules are attached to Amazon CloudWatch events, so that when a rule is triggered it changes the state of the CloudWatch event. You can configure any action on the CloudWatch event, such as to stop the training job saving you time and money. Debugger can be used inside or outside of SageMaker. However the built-in rules that AWS provides are only available for SageMaker training. Scenarios of usage can be classified into the following three cases. Using SageMaker Debugger with Zero Script Change of Your Training Script Here you specify which rules to use when setting up the estimator and run your existing script without no change. For an example of this, see Running a Rule with Zero Script Change on SageMaker . Using SageMaker Debugger on Bring Your Own Container You can use Debugger with your training script on your own container making only a minimal modification to your training script to add Debugger's Hook . For an example template of code to use Debugger on your own container in TensorFlow 2.x frameworks, see Running on Your Own Container . See the following instruction pages to set up Debugger in your preferred framework. - TensorFlow - MXNet - PyTorch - XGBoost Using SageMaker Debugger on a Non-SageMaker Environment Here you write custom rules (or manually analyze the tensors) and modify your training script minimally to enable Debugger on a non-SageMaker Environment such as your local machine. For an example of this, see Running Locally . The reason for different setups is that Zero Script Change (via AWS Deep Learning Containers) uses custom framework forks of TensorFlow, PyTorch, MXNet, and XGBoost which add the Hook to the training job and save requested tensors automatically. These framework forks are not available in custom containers or non-SageMaker environments, so you must modify your training script in these environments. Examples SageMaker Notebook Examples To find a collection of demonstrations using Debugger, see SageMaker Debugger Example Notebooks . Run a Rule with Zero Script Change This example shows a how to use Debugger with Zero Script Change of your training script on a SageMaker DLC. import sagemaker as sm from sagemaker.debugger import rule_configs, Rule, CollectionConfig # Choose a built-in rule to monitor your training job rule = Rule.sagemaker( rule_configs.exploding_tensor(), # configure your rule if applicable rule_parameters={\"tensor_regex\": \".*\"}, # specify collections to save for processing your rule collections_to_save=[ CollectionConfig(name=\"weights\"), CollectionConfig(name=\"losses\"), ], ) # Pass the rule to the estimator sagemaker_simple_estimator = sm.tensorflow.TensorFlow( entry_point=\"script.py\", #replace script.py to your own training script role=sm.get_execution_role(), framework_version=\"1.15\", py_version=\"py3\", # argument for smdebug below rules=[rule], ) sagemaker_simple_estimator.fit() tensors_path = sagemaker_simple_estimator.latest_job_debugger_artifacts_path() import smdebug.trials as smd trial = smd.create_trial(out_dir=tensors_path) print(f\"Saved these tensors: {trial.tensor_names()}\") print(f\"Loss values during evaluation were {trial.tensor('CrossEntropyLoss:0').values(mode=smd.modes.EVAL)}\") That's it! When you configure the sagemaker_simple_estimator , you simply specify the entry_point to your training script python file. When you run the sagemaker_simple_estimator.fit() API, SageMaker will automatically monitor your training job for you with the Rules specified and create a CloudWatch event that tracks the status of the Rule, so you can take any action based on them. If you want additional configuration and control, see Running SageMaker jobs with Debugger for more information. Run Debugger in Your Own Container The following example shows how to set hook to set a training model using Debugger in your own container. This example is for containers in TensorFlow 2.x framework using GradientTape to configure the hook . import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) for epoch in range(n_epochs): for data, labels in dataset: dataset_labels = labels # wrap the tape to capture tensors with hook.wrap_tape(tf.GradientTape(persistent=True)) as tape: logits = model(data, training=True) # (32,10) loss_value = cce(labels, logits) grads = tape.gradient(loss_value, model.variables) opt.apply_gradients(zip(grads, model.variables)) acc = train_acc_metric(dataset_labels, logits) # manually save metric values hook.record_tensor_value(tensor_name=\"accuracy\", tensor_value=acc) To see a full script of this, refer to the tf_keras_gradienttape.py example script. For a notebook example of using BYOC in PyTorch, see Using Amazon SageMaker Debugger with Your Own PyTorch Container Run Debugger Locally Requires Python 3.6+ and this example uses tf.keras. To use Debugger, simply add a callback hook : import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir='~/smd_outputs/') model = tf.keras.models.Sequential([ ... ]) model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', ) # Add the hook as a callback model.fit(x_train, y_train, epochs=2, callbacks=[hook]) model.evaluate(x_test, y_test, callbacks=[hook]) # Create a trial to inspect the saved tensors trial = smd.create_trial(out_dir='~/smd_outputs/') print(f\"Saved these tensors: {trial.tensor_names()}\") print(f\"Loss values during evaluation were {trial.tensor('CrossEntropyLoss:0').values(mode=smd.modes.EVAL)}\") Further Documentation Section Description SageMaker Training SageMaker users, we recommend you start with this page on how to run SageMaker training jobs with SageMaker Debugger Frameworks TensorFlow PyTorch MXNet XGBoost See the frameworks pages for details on what's supported and how to modify your training script if applicable APIs for Saving Tensors Full description of the APIs for saving tensors Programming Model for Analysis For description of the programming model provided by the APIs that enable you to perform interactive exploration of tensors saved as well as to write your own Rules monitoring your training jobs. Release Notes Latest release v0.7.2 Introducing experimental support for TF 2.x training scripts using GradientTape - With this update, weights, bias, loss, metrics, and gradients are captured by SageMaker Debugger. GradientTape in TF 2.x captures these tensors from custom training jobs. An example of GradientTape implementation to a custom ResNet training script using TensorFlow's Keras interface is provided at tf_keras_gradienttape.py . GradientTape does not work with Zero Script Change experience at this time. *Note*: Training scripts using GradientTape for higher-order gradients or multiple tapes are not supported. Distributed training scripts that use GradientTape are not supported at this time. Support SyncOnReadVariable in mirrored strategy - Fixes a bug that occurred because the SyncOnRead distributed variable was not supported with smdebug . Also enables the use of smdebug with training scripts using TF 2.x MirroredStrategy with the fit() API. Turn off hook and write only from one worker for unsupported distributed training techniques \u2013 Fixes a crash when distributed training in PyTorch framework is implemented using generic multiprocessing library, which is not a method supported by smdebug . This fix handles this case and ensures that tensors are saved. Bug fix: Pytorch: Register only if tensors require gradients \u2013 Users were observing a crash when training with pre-trained embeddings which does not need gradient updates. This fix checks if a gradient update is required and registers a backward hook only in those cases. License This library is licensed under the Apache 2.0 License.","title":"Home"},{"location":"#amazon-sagemaker-debugger","text":"","title":"Amazon SageMaker Debugger"},{"location":"#table-of-contents","text":"Overview SageMaker Debugger in action Install How It Works Examples Further Documentation Release Notes","title":"Table of Contents"},{"location":"#overview","text":"Amazon SageMaker Debugger automates the debugging process of machine learning training jobs. From training jobs, Debugger allows you to run your own training script (Zero Script Change experience) using Debugger built-in features\u2014 Hook and Rule \u2014to capture tensors, have flexibility to build customized Hooks and Rules for configuring tensors as you want, and make the tensors available for analysis by saving in an Amazon S3 bucket, all through a flexible and powerful API. The smdebug library powers Debugger by calling the saved tensors from the S3 bucket during the training job. smdebug retrieves and filters the tensors generated from Debugger such as gradients, weights, and biases. Debugger helps you develop better, faster, and cheaper models by minimally modifying estimator, tracing the tensors, catching anomalies while training models, and iterative model pruning. Debugger supports TensorFlow, PyTorch, MXNet, and XGBoost frameworks. The following list is a summary of the main functionalities of Debugger: Zero Script Change experience on SageMaker when using supported containers Full visibility into any tensor part of the training process Real-time training job monitoring through Rules Automated anomaly detection and state assertions through built-in and custom Rules on SageMaker Actions on your training jobs based on the status of Rules Interactive exploration of saved tensors Distributed training support TensorBoard support See How it works for more details.","title":"Overview"},{"location":"#sagemaker-debugger-in-action","text":"Through the model pruning process using Debugger and smdebug , you can iteratively identify the importance of weights and cut neurons below a threshold you define. This process allows you to train the model with significantly fewer neurons, which means a lighter, more efficient, faster, and cheaper model without compromising accuracy. See Using SageMaker Debugger and SageMaker Experiments for iterative model pruning notebook for visualization and further information. Use Debugger with XGBoost in SageMaker Studio to save feature importance values and plot them in a notebook during training. Use Debugger with TensorFlow in SageMaker Studio to run built-in rules and visualize the loss.","title":"SageMaker Debugger in Action"},{"location":"#install-sagemaker-debugger","text":"smdebug library runs on Python 3.x. Install smdebug through: pip install smdebug","title":"Install SageMaker Debugger"},{"location":"#debugger-usage-and-supported-frameworks","text":"There are two ways in which you can enable SageMaker Debugger while training on SageMaker\u2014Zero Script Change and Bring Your Own Training Container (BYOC).","title":"Debugger Usage and Supported Frameworks"},{"location":"#zero-script-change","text":"You can use your own training script while using AWS Deep Learning Containers (DLC) in TensorFlow, PyTorch, MXNet, and XGBoost frameworks. The AWS DLCs enable you to use Debugger with no changes to your training script by automatically adding SageMaker Debugger's Hook . The following table shows currently supported versions of the four frameworks for Zero Script Change experience. Framework Version TensorFlow 1.15, 1.15.2, 2.1 MXNet 1.6 PyTorch 1.3, 1.4 XGBoost >=0.90-2 As Built-in algorithm For the full list and information of the AWS DLCs, see Deep Learning Containers Images .","title":"Zero Script Change"},{"location":"#bring-your-own-training-container","text":"smdebug supports frameworks other than the ones listed in the previous Zero Script Change section. You can use your own training script by adding a minimal modification. Currently supported versions of frameworks are listed in the following table. Framework Versions TensorFlow 1.14. 1.15, 2.0.1, 2.1.0 Keras (with TensorFlow backend) 2.3 MXNet 1.4, 1.5, 1.6 PyTorch 1.2, 1.3, 1.4 XGBoost As Framework","title":"Bring Your Own Training Container"},{"location":"#support-for-distributed-training-and-known-limitations","text":"Distributed Training Horovod Supported TF 1.15, PT 1.4, MX 1.6 Not supported TF 2.x, PT 1.5 Parameter Server-based Not supported","title":"Support for Distributed Training and Known Limitations"},{"location":"#how-it-works","text":"Amazon SageMaker Debugger uses the construct of a Hook to save the values of requested tensors throughout the training process. You can then setup a Rule job which simultaneously monitors and validates these tensors to ensure that training is progressing as expected. A Rule checks for vanishing gradients, exploding tensor values, or poor weight initialization. Rules are attached to Amazon CloudWatch events, so that when a rule is triggered it changes the state of the CloudWatch event. You can configure any action on the CloudWatch event, such as to stop the training job saving you time and money. Debugger can be used inside or outside of SageMaker. However the built-in rules that AWS provides are only available for SageMaker training. Scenarios of usage can be classified into the following three cases.","title":"How It Works"},{"location":"#using-sagemaker-debugger-with-zero-script-change-of-your-training-script","text":"Here you specify which rules to use when setting up the estimator and run your existing script without no change. For an example of this, see Running a Rule with Zero Script Change on SageMaker .","title":"Using SageMaker Debugger with Zero Script Change of Your Training Script"},{"location":"#using-sagemaker-debugger-on-bring-your-own-container","text":"You can use Debugger with your training script on your own container making only a minimal modification to your training script to add Debugger's Hook . For an example template of code to use Debugger on your own container in TensorFlow 2.x frameworks, see Running on Your Own Container . See the following instruction pages to set up Debugger in your preferred framework. - TensorFlow - MXNet - PyTorch - XGBoost","title":"Using SageMaker Debugger on Bring Your Own Container"},{"location":"#using-sagemaker-debugger-on-a-non-sagemaker-environment","text":"Here you write custom rules (or manually analyze the tensors) and modify your training script minimally to enable Debugger on a non-SageMaker Environment such as your local machine. For an example of this, see Running Locally . The reason for different setups is that Zero Script Change (via AWS Deep Learning Containers) uses custom framework forks of TensorFlow, PyTorch, MXNet, and XGBoost which add the Hook to the training job and save requested tensors automatically. These framework forks are not available in custom containers or non-SageMaker environments, so you must modify your training script in these environments.","title":"Using SageMaker Debugger on a Non-SageMaker Environment"},{"location":"#examples","text":"","title":"Examples"},{"location":"#sagemaker-notebook-examples","text":"To find a collection of demonstrations using Debugger, see SageMaker Debugger Example Notebooks .","title":"SageMaker Notebook Examples"},{"location":"#run-a-rule-with-zero-script-change","text":"This example shows a how to use Debugger with Zero Script Change of your training script on a SageMaker DLC. import sagemaker as sm from sagemaker.debugger import rule_configs, Rule, CollectionConfig # Choose a built-in rule to monitor your training job rule = Rule.sagemaker( rule_configs.exploding_tensor(), # configure your rule if applicable rule_parameters={\"tensor_regex\": \".*\"}, # specify collections to save for processing your rule collections_to_save=[ CollectionConfig(name=\"weights\"), CollectionConfig(name=\"losses\"), ], ) # Pass the rule to the estimator sagemaker_simple_estimator = sm.tensorflow.TensorFlow( entry_point=\"script.py\", #replace script.py to your own training script role=sm.get_execution_role(), framework_version=\"1.15\", py_version=\"py3\", # argument for smdebug below rules=[rule], ) sagemaker_simple_estimator.fit() tensors_path = sagemaker_simple_estimator.latest_job_debugger_artifacts_path() import smdebug.trials as smd trial = smd.create_trial(out_dir=tensors_path) print(f\"Saved these tensors: {trial.tensor_names()}\") print(f\"Loss values during evaluation were {trial.tensor('CrossEntropyLoss:0').values(mode=smd.modes.EVAL)}\") That's it! When you configure the sagemaker_simple_estimator , you simply specify the entry_point to your training script python file. When you run the sagemaker_simple_estimator.fit() API, SageMaker will automatically monitor your training job for you with the Rules specified and create a CloudWatch event that tracks the status of the Rule, so you can take any action based on them. If you want additional configuration and control, see Running SageMaker jobs with Debugger for more information.","title":"Run a Rule with Zero Script Change"},{"location":"#run-debugger-in-your-own-container","text":"The following example shows how to set hook to set a training model using Debugger in your own container. This example is for containers in TensorFlow 2.x framework using GradientTape to configure the hook . import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) for epoch in range(n_epochs): for data, labels in dataset: dataset_labels = labels # wrap the tape to capture tensors with hook.wrap_tape(tf.GradientTape(persistent=True)) as tape: logits = model(data, training=True) # (32,10) loss_value = cce(labels, logits) grads = tape.gradient(loss_value, model.variables) opt.apply_gradients(zip(grads, model.variables)) acc = train_acc_metric(dataset_labels, logits) # manually save metric values hook.record_tensor_value(tensor_name=\"accuracy\", tensor_value=acc) To see a full script of this, refer to the tf_keras_gradienttape.py example script. For a notebook example of using BYOC in PyTorch, see Using Amazon SageMaker Debugger with Your Own PyTorch Container","title":"Run Debugger in Your Own Container"},{"location":"#run-debugger-locally","text":"Requires Python 3.6+ and this example uses tf.keras. To use Debugger, simply add a callback hook : import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir='~/smd_outputs/') model = tf.keras.models.Sequential([ ... ]) model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', ) # Add the hook as a callback model.fit(x_train, y_train, epochs=2, callbacks=[hook]) model.evaluate(x_test, y_test, callbacks=[hook]) # Create a trial to inspect the saved tensors trial = smd.create_trial(out_dir='~/smd_outputs/') print(f\"Saved these tensors: {trial.tensor_names()}\") print(f\"Loss values during evaluation were {trial.tensor('CrossEntropyLoss:0').values(mode=smd.modes.EVAL)}\")","title":"Run Debugger Locally"},{"location":"#further-documentation","text":"Section Description SageMaker Training SageMaker users, we recommend you start with this page on how to run SageMaker training jobs with SageMaker Debugger Frameworks TensorFlow PyTorch MXNet XGBoost See the frameworks pages for details on what's supported and how to modify your training script if applicable APIs for Saving Tensors Full description of the APIs for saving tensors Programming Model for Analysis For description of the programming model provided by the APIs that enable you to perform interactive exploration of tensors saved as well as to write your own Rules monitoring your training jobs.","title":"Further Documentation"},{"location":"#release-notes","text":"","title":"Release Notes"},{"location":"#latest-release-v072","text":"Introducing experimental support for TF 2.x training scripts using GradientTape - With this update, weights, bias, loss, metrics, and gradients are captured by SageMaker Debugger. GradientTape in TF 2.x captures these tensors from custom training jobs. An example of GradientTape implementation to a custom ResNet training script using TensorFlow's Keras interface is provided at tf_keras_gradienttape.py . GradientTape does not work with Zero Script Change experience at this time. *Note*: Training scripts using GradientTape for higher-order gradients or multiple tapes are not supported. Distributed training scripts that use GradientTape are not supported at this time. Support SyncOnReadVariable in mirrored strategy - Fixes a bug that occurred because the SyncOnRead distributed variable was not supported with smdebug . Also enables the use of smdebug with training scripts using TF 2.x MirroredStrategy with the fit() API. Turn off hook and write only from one worker for unsupported distributed training techniques \u2013 Fixes a crash when distributed training in PyTorch framework is implemented using generic multiprocessing library, which is not a method supported by smdebug . This fix handles this case and ensures that tensors are saved. Bug fix: Pytorch: Register only if tensors require gradients \u2013 Users were observing a crash when training with pre-trained embeddings which does not need gradient updates. This fix checks if a gradient update is required and registers a backward hook only in those cases.","title":"Latest release v0.7.2"},{"location":"#license","text":"This library is licensed under the Apache 2.0 License.","title":"License"},{"location":"analysis/","text":"Programming Model for Analysis This page describes the programming model that SageMaker Debugger provides for your analysis, and introduces you to the constructs of Trial, Tensor and Rule. Table of Contents Trial Path of trial SageMaker training job Non SageMaker training jobs Creating a trial object Creating S3 trial Creating local trial Restricting analysis to a range of steps Trial API tensor_names tensor has_tensor steps modes mode mode_step global_step workers collections collection wait_for_steps has_passed_step Tensor Tensor API steps value reduction_value reduction_values values workers prev_steps Rules Built In Rules Writing a custom rule Constructor Function to invoke at a given step Invoking a rule invoke_rule Exceptions Utils Enable or disable refresh of tensors in a trial Trial Trial is an object which lets you query for tensors for a given training job, specified by the path where smdebug's artifacts are saved. Trial is capable of loading new tensors as and when they become available at the given path, allowing you to do both offline as well as realtime analysis. Path of trial SageMaker training job When running a SageMaker job this path is on S3. SageMaker saves data from your training job locally on the training instance first and uploads them to an S3 location in your account. When you start a SageMaker training job with the python SDK, you can control this path using the parameter s3_output_path in the DebuggerHookConfig object. This is an optional parameter, if you do not pass this the python SDK will populate a default location for you. If you do pass this, make sure the bucket is in the same region as where the training job is running. If you're not using the python SDK, set this path for the parameter S3OutputPath in the DebugHookConfig section of CreateTrainingJob API. SageMaker takes this path and appends training_job_name and \"debug-output\" to it to ensure we have a unique path for each training job. Non SageMaker training jobs If you are not running a SageMaker training job, this is the path you pass as out_dir when you create a smdebug Hook . Just like when creating the hook, you can pass either a local path or an S3 path (as s3://bucket/prefix ). Creating a trial object There are two types of trials you can create: LocalTrial or S3Trial depending on the path. We provide a wrapper method to create the appropriate trial. The parameters you have to provide are: - path : path can be a local path or an S3 path of the form s3://bucket/prefix . You should see directories such as collections , events and index at this path once the training job starts. - name : name can be any string. It is to help you manage different trials. This is an optional parameter, which defaults to the basename of the path if not passed. Please make sure to give it a unique name to prevent confusion. Creating S3 trial from smdebug.trials import create_trial trial = create_trial(path='s3://smdebug-testing-bucket/outputs/resnet', name='resnet_training_run') Creating local trial from smdebug.trials import create_trial trial = create_trial(path='/home/ubuntu/smdebug_outputs/resnet', name='resnet_training_run') Restricting analysis to a range of steps You can optionally pass range_steps to restrict your analysis to a certain range of steps. Note that if you do so, Trial will not load data from other steps. Examples - range_steps=(100, None) : This will load all steps after 100 - range_steps=(None, 100) : This will load all steps before 100 - range_steps=(100, 200) : This will load steps between 100 and 200 - range_steps=None : This will load all steps from smdebug.trials import create_trial tr = create_trial(path='s3://smdebug-testing-bucket/outputs/resnet', name='resnet_training', range_steps=(100, 200)) Trial API Here's a list of methods that the Trial API provides which helps you load data for analysis. Please click on the method to see all the parameters it takes and a detailed description. If you are not familiar with smdebug constructs, you might want to review this doc before going through this page. Method Description trial.tensor_names() See names of all tensors available trial.tensor(name) Retrieve smdebug Tensor object trial.has_tensor(name) Query for whether tensor was saved trial.steps() Query steps for which data was saved trial.modes() Query modes for which data was saved trial.mode(step) Query the mode for a given global step trial.global_step(mode, step) Query global step for a given step and mode trial.mode_step(step) Query the mode step for a given global step trial.workers() Query list of workers from the data saved trial.collections() Query list of collections saved from the training job trial.collection(name) Retrieve a single collection saved from the training job trial.wait_for_steps(steps) Wait till the requested steps are available trial.has_passed_step(step) Query whether the requested step is available tensor_names Retrieves names of tensors saved trial.tensor_names(step= None, mode=modes.GLOBAL, regex=None, collection=None) Arguments All arguments to this method are optional. You are not required to pass any of these arguments as keyword arguments. step (int) If you want to retrieve the list of tensors saved at a particular step, pass the step number as an integer. This step number will be treated as step number corresponding to the mode passed below. By default it is treated as global step. mode (smdebug.modes enum value) If you want to retrieve the list of tensors saved for a particular mode, pass the mode here as smd.modes.TRAIN , smd.modes.EVAL , smd.modes.PREDICT , or smd.modes.GLOBAL . regex (str or list[str]) You can filter tensors matching regex expressions by passing a regex expressions as a string or list of strings. You can only pass one of regex or collection parameters. collection (Collection or str) You can filter tensors belonging to a collection by either passing a collection object or the name of collection as a string. You can only pass one of regex or collection parameters. Returns list[str] : List of strings representing names of tensors matching the given arguments. Arguments are processed as follows: get the list of tensor names for given step and mode, saved for given step matching all the given arguments, i.e. intersection of tensors matching each of the parameters. Examples trial.tensor_names() Returns all tensors saved for any step or mode. trial.tensor_names(step=10, mode=modes.TRAIN) Returns tensors saved for training step 10 trial.tensor_names(regex='relu') Returns all tensors matching the regex pattern relu saved for any step or mode. trial.tensor_names(collection='gradients') Returns tensors from collection \"gradients\" trial.tensor_names(step=10, mode=modes.TRAIN, regex='softmax') Returns tensor saved for 10th training step which matches the regex softmax tensor Retrieve the smdebug.core.tensor.Tensor object by the given name tname . You can review all the methods that this Tensor object provides here . trial.tensor(tname) Arguments tname (str) Takes the name of tensor Returns smdebug.core.tensor.Tensor object which has this API has_tensor Query whether the trial has a tensor by the given name trial.has_tensor(tname) Arguments tname (str) Takes the name of tensor Returns bool : True if the tensor is seen by the trial so far, else False . steps Retrieve a list of steps seen by the trial trial.steps(mode=None) Arguments mode (smdebug.modes enum value) Passing a mode here allows you want to retrieve the list of steps seen by a trial for that mode If this is not passed, returns steps for all modes. Returns list[int] List of integers representing step numbers. If a mode was passed, this returns steps within that mode, i.e. mode steps. Each of these mode steps has a global step number associated with it. The global step represents the sequence of steps across all modes executed by the job. modes Retrieve a list of modes seen by the trial trial.modes() Returns list[smdebug.modes enum value] List of modes for which data was saved from the training job across all steps seen. mode Given a global step number you can identify the mode for that step using this method. trial.mode(global_step=100) Arguments global_step (int) Takes the global step as an integer Returns smdebug.modes enum value of the given global step mode_step Given a global step number you can identify the mode_step for that step using this method. trial.mode_step(global_step=100) Arguments global_step (int) Takes the global step as an integer Returns int : An integer representing mode_step of the given global step. Typically used in conjunction with mode method. global_step Given a mode and a mode_step number you can retrieve its global step using this method. trial.global_step(mode=modes.GLOBAL, mode_step=100) Arguments mode (smdebug.modes enum value) Takes the mode as enum value mode_step (int) Takes the mode step as an integer Returns int An integer representing global_step of the given mode and mode_step. workers Query for all the worker processes from which data was saved by smdebug during multi worker training. trial.workers() Returns list[str] A sorted list of names of worker processes from which data was saved. If using TensorFlow Mirrored Strategy for multi worker training, these represent names of different devices in the process. For Horovod, torch.distributed and similar distributed training approaches, these represent names of the form worker_0 where 0 is the rank of the process. collections List the collections from the trial. Note that tensors part of these collections may not necessarily have been saved from the training job. Whether a collection was saved or not depends on the configuration of the Hook during training. trial.collections() Returns dict[str -> Collection] A dictionary indexed by the name of the collection, with the Collection object as the value. Please refer Collection API for more details. collection Get a specific collection from the trial. Note that tensors which are part of this collection may not necessarily have been saved from the training job. Whether this collection was saved or not depends on the configuration of the Hook during training. trial.collection(coll_name) Arguments coll_name (str) Name of the collection Returns Collection The requested Collection object. Please refer Collection API for more details. wait_for_steps This method allows you to wait for steps before proceeding. You might want to use this method if you want to wait for smdebug to see the required steps so you can then query and analyze the tensors saved by that step. This method blocks till all data from the steps are seen by smdebug. trial.wait_for_steps(required_steps, mode=modes.GLOBAL) Arguments required_steps (list[int]) Step numbers to wait for mode (smdebug.modes enum value) The mode to which given step numbers correspond to. This defaults to modes.GLOBAL. Returns None, but it only returns after we know definitely whether we have seen the steps. Exceptions raised StepUnavailable and NoMoreData . See Exceptions section for more details. has_passed_step trial.has_passed_step(step, mode=modes.GLOBAL) Arguments step (int) The step number to check if the trial has passed it mode (smdebug.modes enum value) The mode to which given step number corresponds to. This defaults to modes.GLOBAL. Returns smdebug.core.tensor.StepState enum value which can take one of three values UNAVAILABLE , AVAILABLE and NOT_YET_AVAILABLE . TODO@Nihal describe these in detail Tensor An smdebug Tensor object can be retrieved through the trial.tensor(name) API. It is uniquely identified by the string representing name. It provides the following methods. Method Description steps() Query steps for which tensor was saved value(step) Get the value of the tensor at a given step as a numpy array reduction_value(step) Get the reduction value of the chosen tensor at a particular step reduction_values(step) Get all reduction values saved for the chosen tensor at a particular step values(mode) Get the values of the tensor for all steps of a given mode workers(step) Get all the workers for which this tensor was saved at a given step prev_steps(step, n) Get the last n step numbers of a given mode from a given step Tensor API steps Query for the steps at which the given tensor was saved trial.tensor(name).steps(mode=ModeKeys.GLOBAL, show_incomplete_steps=False) Arguments mode (smdebug.modes enum value) The mode whose steps to return for the given tensor. Defaults to modes.GLOBAL show_incomplete_steps (bool) This parameter is relevant only for distributed training. By default this method only returns the steps which have been received from all workers. But if this parameter is set to True, this method will return steps received from at least one worker. Returns list[int] A list of steps at which the given tensor was saved value Get the value of the tensor at a given step as a numpy array trial.tensor(name).value(step_num, mode=ModeKeys.GLOBAL, worker=None) Arguments step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode) Returns numpy.ndarray The value of tensor at the given step and worker (if the training job saved data from multiple workers) reduction_value Get the reduction value of the chosen tensor at a particular step. A reduction value is a tensor reduced to a single value through reduction or aggregation operations. The different reductions you can query for are the same as what are allowed in ReductionConfig when saving tensors. This API thus allows you to access the reduction you might have saved instead of the full tensor. If you had saved the full tensor, it will calculate the requested reduction at the time of this call. Reduction names allowed are min , max , mean , prod , std , sum , variance and l1 , l2 representing the norms. Each of these can be retrieved for the absolute value of the tensor or the original tensor. Above was an example to get the mean of the absolute value of the tensor. abs can be set to False if you want to see the mean of the actual tensor. If you had saved the tensor without any reduction, then you can retrieve the actual tensor as a numpy array and compute any reduction you might be interested in. In such a case you do not need this method. trial.tensor(name).reduction_value(step_num, reduction_name, mode=modes.GLOBAL, worker=None, abs=False) Arguments step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. reduction_name (str) The name of the reduction to query for. This can be one of min , max , mean , std , variance , sum , prod and the norms l1 , l2 . mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode) abs (bool) If abs is True, this method tries to return the reduction passed through reduction_name after taking the absolute value of the tensor. It defaults to False . Returns numpy.ndarray The reduction value of tensor at the given step and worker (if the training job saved data from multiple workers) as a 1x1 numpy array. If this reduction was saved for the tensor during training as part of specification through reduction config, it will be loaded and returned. If the given reduction was not saved then, but the full tensor was saved, the reduction will be computed on the fly and returned. If both the chosen reduction and full tensor are not available, this method raises TensorUnavailableForStep exception. reduction_values Get all reduction values saved for the chosen tensor at a particular step. A reduction value is a tensor reduced to a single value through reduction or aggregation operations. Please go through the description of the method reduction_value for more details. trial.tensor(name).reduction_values(step_num, mode=modes.GLOBAL, worker=None) Arguments step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode) Returns dict[(str, bool) -> numpy.ndarray] A dictionary with keys being tuples of the form (reduction_name, abs) to a 1x1 numpy ndarray value. abs here is a boolean that denotes whether the reduction was performed on the absolute value of the tensor or not. Note that this method only returns the reductions which were saved from the training job. It does not compute all known reductions and return them if only the raw tensor was saved. values Get the values of the tensor for all steps of a given mode. trial.tensor(name).values(mode=modes.GLOBAL, worker=None) Arguments mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode) Returns dict[int -> numpy.ndarray] A dictionary with step numbers as keys and numpy arrays representing the value of the tensor as values. workers Get all the workers for which this tensor was saved at a given step trial.tensor(name).workers(step_num, mode=modes.GLOBAL) Arguments step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL Returns list[str] A list of worker names for which the tensor was saved at the given step. prev_steps Get the last n step numbers of a given mode from a given step. trial.tensor(name).prev_steps(step, n, mode=modes.GLOBAL) Arguments step (int) The step number whose value is to be returned for the mode passed. n (int) Number of previous steps to return mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL Returns list[int] A list of size at most n representing the previous steps for the given step and mode. Note that this list can be of size less than n if there were only less than n steps saved before the given step in this trial. Rules Rules are the medium by which SageMaker Debugger executes a certain piece of code regularly on different steps of a training job. A rule is assigned to a trial and can be invoked at each new step of the trial. It can also access other trials for its evaluation. You can evaluate a rule using tensors from the current step or any step before the current step. Please ensure your logic respects these semantics, else you will get a TensorUnavailableForStep exception as the data would not yet be available for future steps. Built In Rules Please refer to the built-in rules that SageMaker provides here . Writing a custom rule Writing a rule involves implementing the Rule interface . Below, let us look at a simplified version of a VanishingGradient rule. Constructor Creating a rule involves first inheriting from the base Rule class provided by smdebug. For this example rule here, we do not need to look at any other trials, so we set other_trials to None. from smdebug.rules import Rule class VanishingGradientRule(Rule): def __init__(self, base_trial, threshold=0.0000001): super().__init__(base_trial, other_trials=None) self.threshold = float(threshold) Please note that apart from base_trial and other_trials (if required), we require all arguments of the rule constructor to take a string as value. You can parse them to the type that you want from the string. This means if you want to pass a list of strings, you might want to pass them as a comma separated string. This restriction is being enforced so as to let you create and invoke rules from json using Sagemaker's APIs. Function to invoke at a given step In this function you can implement the core logic of what you want to do with these tensors. It should return a boolean value True or False , where True means the rule evaluation condition has been met. When you invoke these rules through SageMaker, the rule evaluation ends when the rule evaluation condition is met. SageMaker creates a Cloudwatch event for every rule evaluation job, which can be used to define actions that you might want to take based on the state of the rule. A simplified version of the actual invoke function for VanishingGradientRule is below: def invoke_at_step(self, step): for tensorname in self.base_trial.tensors(collection='gradients'): tensor = self.base_trial.tensor(tensorname) abs_mean = tensor.reduction_value(step, 'mean', abs=True) if abs_mean < self.threshold: return True else: return False That's it, writing a rule is as simple as that. Invoking a rule through SageMaker After you've written your rule, you can ask SageMaker to evaluate the rule against your training job by either using SageMaker Python SDK as estimator = Estimator( ... rules = Rules.custom( name='VGRule', image_uri='864354269164.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', instance_type='ml.t3.medium', # instance type to run the rule evaluation on source='rules/vanishing_gradient_rule.py', # path to the rule source file rule_to_invoke='VanishingGradientRule', # name of the class to invoke in the rule source file volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance collections_to_save=[CollectionConfig(\"gradients\")], # collections to be analyzed by the rule rule_parameters={ \"threshold\": \"20.0\" # this will be used to initialize 'threshold' param in your rule constructor } ) If you're using the SageMaker API directly to evaluate the rule, then you can specify the rule configuration DebugRuleConfigurations in the CreateTrainingJob API request as: \"DebugRuleConfigurations\": [ { \"RuleConfigurationName\": \"VGRule\", \"InstanceType\": \"ml.t3.medium\", \"VolumeSizeInGB\": 30, \"RuleEvaluatorImage\": \"864354269164.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest\", \"RuleParameters\": { \"source_s3_uri\": \"s3://path/to/vanishing_gradient_rule.py\", \"rule_to_invoke\": \"VanishingGradient\", \"threshold\": \"20.0\" } } ] Invoking a rule outside of SageMaker through invoke_rule You might want to invoke the rule locally during development. We provide a function to invoke rules easily. Refer smdebug/rules/rule_invoker.py . The invoke function has the following syntax. It takes a instance of a Rule and invokes it for a series of steps one after the other. from smdebug.rules import invoke_rule from smdebug.trials import create_trial trial = create_trial('s3://smdebug-dev-test/mnist-job/') rule_obj = VanishingGradientRule(trial, threshold=0.0001) invoke_rule(rule_obj, start_step=0, end_step=None) Arguments rule_obj (Rule) An instance of a subclass of smdebug.rules.Rule that you want to invoke. start_step (int) Global step number to start invoking the rule from. Note that this refers to a global step. This defaults to 0. end_step (int or None) : Global step number to end the invocation of rule before. To clarify, end_step is an exclusive bound. The rule is invoked at end_step . This defaults to None which means run till the end of the job. raise_eval_cond (bool) This parameter controls whether to raise the exception RuleEvaluationConditionMet when raised by the rule, or to catch it and log the message and move to the next step. Defaults to False , which implies that the it catches the exception, logs that the evaluation condition was met for a step and moves on to evaluate the next step. Exceptions smdebug is designed to be aware that tensors required to evaluate a rule may not be available at every step. Hence, it raises a few exceptions which allow us to control what happens when a tensor is missing. These are available in the smdebug.exceptions module. You can import them as follows: from smdebug.exceptions import * Here are the exceptions (along with others) and their meaning: TensorUnavailableForStep : This means that the tensor requested is not available for the step. It may have been or will be saved for a different step number. You can check which steps tensor is saved for by trial.tensor('tname').steps() api . Note that this exception implies that the requested tensor will never become available for this step in the future. TensorUnavailable : This means that this tensor has not been saved from the training job. Note that if you have a SaveConfig which saves a certain tensor only after the time you queried for the tensor, you might get a TensorUnavailable exception even if the tensor may become available later for some step. StepUnavailable : This means that the step was not saved from the training job. No tensor will be available for this step. StepNotYetAvailable : This means that the step has not yet been seen from the training job. It may be available in the future if the training is still going on. We automatically load new data as and when it becomes available. This step may either become available in the future, or the exception might change to StepUnavailable . NoMoreData : This will be raised when the training ends. Once you see this, you will know that there will be no more steps and no more tensors saved. RuleEvaluationConditionMet : This is raised when the rule invocation returns True for some step. MissingCollectionFiles : This is raised when no data was saved by the training job. Check that the Hook was configured correctly before starting the training job. Utils Enable or disable refresh of tensors in a trial By default smdebug refreshes tensors each time you try to query the tensor. It looks for whether this tensor is saved for new steps and if so fetches them. If you know the saved data will not change (stopped the machine learning job), or are not interested in the latest data, you can stop the refreshing of tensors as follows: no_refresh takes a trial or a list of trials, which should not be refreshed. Anything executed inside the with no_refresh block will not be refreshed. from smdebug.analysis.utils import no_refresh with no_refresh(trials): pass Similarly if you want to refresh tensors only within a block, you can do: from smdebug.analysis.utils import refresh with refresh(trials): pass During rule invocation smdebug waits till the current step is available and then turns off refresh to ensure that you do not get different results for methods like trial.tensor(name).steps() and run into subtle issues.","title":"Programming Model for Analysis"},{"location":"analysis/#programming-model-for-analysis","text":"This page describes the programming model that SageMaker Debugger provides for your analysis, and introduces you to the constructs of Trial, Tensor and Rule.","title":"Programming Model for Analysis"},{"location":"analysis/#table-of-contents","text":"Trial Path of trial SageMaker training job Non SageMaker training jobs Creating a trial object Creating S3 trial Creating local trial Restricting analysis to a range of steps Trial API tensor_names tensor has_tensor steps modes mode mode_step global_step workers collections collection wait_for_steps has_passed_step Tensor Tensor API steps value reduction_value reduction_values values workers prev_steps Rules Built In Rules Writing a custom rule Constructor Function to invoke at a given step Invoking a rule invoke_rule Exceptions Utils Enable or disable refresh of tensors in a trial","title":"Table of Contents"},{"location":"analysis/#trial","text":"Trial is an object which lets you query for tensors for a given training job, specified by the path where smdebug's artifacts are saved. Trial is capable of loading new tensors as and when they become available at the given path, allowing you to do both offline as well as realtime analysis.","title":"Trial"},{"location":"analysis/#path-of-trial","text":"","title":"Path of trial"},{"location":"analysis/#sagemaker-training-job","text":"When running a SageMaker job this path is on S3. SageMaker saves data from your training job locally on the training instance first and uploads them to an S3 location in your account. When you start a SageMaker training job with the python SDK, you can control this path using the parameter s3_output_path in the DebuggerHookConfig object. This is an optional parameter, if you do not pass this the python SDK will populate a default location for you. If you do pass this, make sure the bucket is in the same region as where the training job is running. If you're not using the python SDK, set this path for the parameter S3OutputPath in the DebugHookConfig section of CreateTrainingJob API. SageMaker takes this path and appends training_job_name and \"debug-output\" to it to ensure we have a unique path for each training job.","title":"SageMaker training job"},{"location":"analysis/#non-sagemaker-training-jobs","text":"If you are not running a SageMaker training job, this is the path you pass as out_dir when you create a smdebug Hook . Just like when creating the hook, you can pass either a local path or an S3 path (as s3://bucket/prefix ).","title":"Non SageMaker training jobs"},{"location":"analysis/#creating-a-trial-object","text":"There are two types of trials you can create: LocalTrial or S3Trial depending on the path. We provide a wrapper method to create the appropriate trial. The parameters you have to provide are: - path : path can be a local path or an S3 path of the form s3://bucket/prefix . You should see directories such as collections , events and index at this path once the training job starts. - name : name can be any string. It is to help you manage different trials. This is an optional parameter, which defaults to the basename of the path if not passed. Please make sure to give it a unique name to prevent confusion.","title":"Creating a trial object"},{"location":"analysis/#creating-s3-trial","text":"from smdebug.trials import create_trial trial = create_trial(path='s3://smdebug-testing-bucket/outputs/resnet', name='resnet_training_run')","title":"Creating S3 trial"},{"location":"analysis/#creating-local-trial","text":"from smdebug.trials import create_trial trial = create_trial(path='/home/ubuntu/smdebug_outputs/resnet', name='resnet_training_run')","title":"Creating local trial"},{"location":"analysis/#restricting-analysis-to-a-range-of-steps","text":"You can optionally pass range_steps to restrict your analysis to a certain range of steps. Note that if you do so, Trial will not load data from other steps. Examples - range_steps=(100, None) : This will load all steps after 100 - range_steps=(None, 100) : This will load all steps before 100 - range_steps=(100, 200) : This will load steps between 100 and 200 - range_steps=None : This will load all steps from smdebug.trials import create_trial tr = create_trial(path='s3://smdebug-testing-bucket/outputs/resnet', name='resnet_training', range_steps=(100, 200))","title":"Restricting analysis to a range of steps"},{"location":"analysis/#trial-api","text":"Here's a list of methods that the Trial API provides which helps you load data for analysis. Please click on the method to see all the parameters it takes and a detailed description. If you are not familiar with smdebug constructs, you might want to review this doc before going through this page. Method Description trial.tensor_names() See names of all tensors available trial.tensor(name) Retrieve smdebug Tensor object trial.has_tensor(name) Query for whether tensor was saved trial.steps() Query steps for which data was saved trial.modes() Query modes for which data was saved trial.mode(step) Query the mode for a given global step trial.global_step(mode, step) Query global step for a given step and mode trial.mode_step(step) Query the mode step for a given global step trial.workers() Query list of workers from the data saved trial.collections() Query list of collections saved from the training job trial.collection(name) Retrieve a single collection saved from the training job trial.wait_for_steps(steps) Wait till the requested steps are available trial.has_passed_step(step) Query whether the requested step is available","title":"Trial API"},{"location":"analysis/#tensor_names","text":"Retrieves names of tensors saved trial.tensor_names(step= None, mode=modes.GLOBAL, regex=None, collection=None)","title":"tensor_names"},{"location":"analysis/#arguments","text":"All arguments to this method are optional. You are not required to pass any of these arguments as keyword arguments. step (int) If you want to retrieve the list of tensors saved at a particular step, pass the step number as an integer. This step number will be treated as step number corresponding to the mode passed below. By default it is treated as global step. mode (smdebug.modes enum value) If you want to retrieve the list of tensors saved for a particular mode, pass the mode here as smd.modes.TRAIN , smd.modes.EVAL , smd.modes.PREDICT , or smd.modes.GLOBAL . regex (str or list[str]) You can filter tensors matching regex expressions by passing a regex expressions as a string or list of strings. You can only pass one of regex or collection parameters. collection (Collection or str) You can filter tensors belonging to a collection by either passing a collection object or the name of collection as a string. You can only pass one of regex or collection parameters.","title":"Arguments"},{"location":"analysis/#returns","text":"list[str] : List of strings representing names of tensors matching the given arguments. Arguments are processed as follows: get the list of tensor names for given step and mode, saved for given step matching all the given arguments, i.e. intersection of tensors matching each of the parameters.","title":"Returns"},{"location":"analysis/#examples","text":"trial.tensor_names() Returns all tensors saved for any step or mode. trial.tensor_names(step=10, mode=modes.TRAIN) Returns tensors saved for training step 10 trial.tensor_names(regex='relu') Returns all tensors matching the regex pattern relu saved for any step or mode. trial.tensor_names(collection='gradients') Returns tensors from collection \"gradients\" trial.tensor_names(step=10, mode=modes.TRAIN, regex='softmax') Returns tensor saved for 10th training step which matches the regex softmax","title":"Examples"},{"location":"analysis/#tensor","text":"Retrieve the smdebug.core.tensor.Tensor object by the given name tname . You can review all the methods that this Tensor object provides here . trial.tensor(tname)","title":"tensor"},{"location":"analysis/#arguments_1","text":"tname (str) Takes the name of tensor","title":"Arguments"},{"location":"analysis/#returns_1","text":"smdebug.core.tensor.Tensor object which has this API","title":"Returns"},{"location":"analysis/#has_tensor","text":"Query whether the trial has a tensor by the given name trial.has_tensor(tname)","title":"has_tensor"},{"location":"analysis/#arguments_2","text":"tname (str) Takes the name of tensor","title":"Arguments"},{"location":"analysis/#returns_2","text":"bool : True if the tensor is seen by the trial so far, else False .","title":"Returns"},{"location":"analysis/#steps","text":"Retrieve a list of steps seen by the trial trial.steps(mode=None)","title":"steps"},{"location":"analysis/#arguments_3","text":"mode (smdebug.modes enum value) Passing a mode here allows you want to retrieve the list of steps seen by a trial for that mode If this is not passed, returns steps for all modes.","title":"Arguments"},{"location":"analysis/#returns_3","text":"list[int] List of integers representing step numbers. If a mode was passed, this returns steps within that mode, i.e. mode steps. Each of these mode steps has a global step number associated with it. The global step represents the sequence of steps across all modes executed by the job.","title":"Returns"},{"location":"analysis/#modes","text":"Retrieve a list of modes seen by the trial trial.modes()","title":"modes"},{"location":"analysis/#returns_4","text":"list[smdebug.modes enum value] List of modes for which data was saved from the training job across all steps seen.","title":"Returns"},{"location":"analysis/#mode","text":"Given a global step number you can identify the mode for that step using this method. trial.mode(global_step=100)","title":"mode"},{"location":"analysis/#arguments_4","text":"global_step (int) Takes the global step as an integer","title":"Arguments"},{"location":"analysis/#returns_5","text":"smdebug.modes enum value of the given global step","title":"Returns"},{"location":"analysis/#mode_step","text":"Given a global step number you can identify the mode_step for that step using this method. trial.mode_step(global_step=100)","title":"mode_step"},{"location":"analysis/#arguments_5","text":"global_step (int) Takes the global step as an integer","title":"Arguments"},{"location":"analysis/#returns_6","text":"int : An integer representing mode_step of the given global step. Typically used in conjunction with mode method.","title":"Returns"},{"location":"analysis/#global_step","text":"Given a mode and a mode_step number you can retrieve its global step using this method. trial.global_step(mode=modes.GLOBAL, mode_step=100)","title":"global_step"},{"location":"analysis/#arguments_6","text":"mode (smdebug.modes enum value) Takes the mode as enum value mode_step (int) Takes the mode step as an integer","title":"Arguments"},{"location":"analysis/#returns_7","text":"int An integer representing global_step of the given mode and mode_step.","title":"Returns"},{"location":"analysis/#workers","text":"Query for all the worker processes from which data was saved by smdebug during multi worker training. trial.workers()","title":"workers"},{"location":"analysis/#returns_8","text":"list[str] A sorted list of names of worker processes from which data was saved. If using TensorFlow Mirrored Strategy for multi worker training, these represent names of different devices in the process. For Horovod, torch.distributed and similar distributed training approaches, these represent names of the form worker_0 where 0 is the rank of the process.","title":"Returns"},{"location":"analysis/#collections","text":"List the collections from the trial. Note that tensors part of these collections may not necessarily have been saved from the training job. Whether a collection was saved or not depends on the configuration of the Hook during training. trial.collections()","title":"collections"},{"location":"analysis/#returns_9","text":"dict[str -> Collection] A dictionary indexed by the name of the collection, with the Collection object as the value. Please refer Collection API for more details.","title":"Returns"},{"location":"analysis/#collection","text":"Get a specific collection from the trial. Note that tensors which are part of this collection may not necessarily have been saved from the training job. Whether this collection was saved or not depends on the configuration of the Hook during training. trial.collection(coll_name)","title":"collection"},{"location":"analysis/#arguments_7","text":"coll_name (str) Name of the collection","title":"Arguments"},{"location":"analysis/#returns_10","text":"Collection The requested Collection object. Please refer Collection API for more details.","title":"Returns"},{"location":"analysis/#wait_for_steps","text":"This method allows you to wait for steps before proceeding. You might want to use this method if you want to wait for smdebug to see the required steps so you can then query and analyze the tensors saved by that step. This method blocks till all data from the steps are seen by smdebug. trial.wait_for_steps(required_steps, mode=modes.GLOBAL)","title":"wait_for_steps"},{"location":"analysis/#arguments_8","text":"required_steps (list[int]) Step numbers to wait for mode (smdebug.modes enum value) The mode to which given step numbers correspond to. This defaults to modes.GLOBAL.","title":"Arguments"},{"location":"analysis/#returns_11","text":"None, but it only returns after we know definitely whether we have seen the steps.","title":"Returns"},{"location":"analysis/#exceptions-raised","text":"StepUnavailable and NoMoreData . See Exceptions section for more details.","title":"Exceptions raised"},{"location":"analysis/#has_passed_step","text":"trial.has_passed_step(step, mode=modes.GLOBAL)","title":"has_passed_step"},{"location":"analysis/#arguments_9","text":"step (int) The step number to check if the trial has passed it mode (smdebug.modes enum value) The mode to which given step number corresponds to. This defaults to modes.GLOBAL.","title":"Arguments"},{"location":"analysis/#returns_12","text":"smdebug.core.tensor.StepState enum value which can take one of three values UNAVAILABLE , AVAILABLE and NOT_YET_AVAILABLE . TODO@Nihal describe these in detail","title":"Returns"},{"location":"analysis/#tensor_1","text":"An smdebug Tensor object can be retrieved through the trial.tensor(name) API. It is uniquely identified by the string representing name. It provides the following methods. Method Description steps() Query steps for which tensor was saved value(step) Get the value of the tensor at a given step as a numpy array reduction_value(step) Get the reduction value of the chosen tensor at a particular step reduction_values(step) Get all reduction values saved for the chosen tensor at a particular step values(mode) Get the values of the tensor for all steps of a given mode workers(step) Get all the workers for which this tensor was saved at a given step prev_steps(step, n) Get the last n step numbers of a given mode from a given step","title":"Tensor"},{"location":"analysis/#tensor-api","text":"","title":"Tensor API"},{"location":"analysis/#steps_1","text":"Query for the steps at which the given tensor was saved trial.tensor(name).steps(mode=ModeKeys.GLOBAL, show_incomplete_steps=False)","title":"steps"},{"location":"analysis/#arguments_10","text":"mode (smdebug.modes enum value) The mode whose steps to return for the given tensor. Defaults to modes.GLOBAL show_incomplete_steps (bool) This parameter is relevant only for distributed training. By default this method only returns the steps which have been received from all workers. But if this parameter is set to True, this method will return steps received from at least one worker.","title":"Arguments"},{"location":"analysis/#returns_13","text":"list[int] A list of steps at which the given tensor was saved","title":"Returns"},{"location":"analysis/#value","text":"Get the value of the tensor at a given step as a numpy array trial.tensor(name).value(step_num, mode=ModeKeys.GLOBAL, worker=None)","title":"value"},{"location":"analysis/#arguments_11","text":"step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode)","title":"Arguments"},{"location":"analysis/#returns_14","text":"numpy.ndarray The value of tensor at the given step and worker (if the training job saved data from multiple workers)","title":"Returns"},{"location":"analysis/#reduction_value","text":"Get the reduction value of the chosen tensor at a particular step. A reduction value is a tensor reduced to a single value through reduction or aggregation operations. The different reductions you can query for are the same as what are allowed in ReductionConfig when saving tensors. This API thus allows you to access the reduction you might have saved instead of the full tensor. If you had saved the full tensor, it will calculate the requested reduction at the time of this call. Reduction names allowed are min , max , mean , prod , std , sum , variance and l1 , l2 representing the norms. Each of these can be retrieved for the absolute value of the tensor or the original tensor. Above was an example to get the mean of the absolute value of the tensor. abs can be set to False if you want to see the mean of the actual tensor. If you had saved the tensor without any reduction, then you can retrieve the actual tensor as a numpy array and compute any reduction you might be interested in. In such a case you do not need this method. trial.tensor(name).reduction_value(step_num, reduction_name, mode=modes.GLOBAL, worker=None, abs=False)","title":"reduction_value"},{"location":"analysis/#arguments_12","text":"step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. reduction_name (str) The name of the reduction to query for. This can be one of min , max , mean , std , variance , sum , prod and the norms l1 , l2 . mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode) abs (bool) If abs is True, this method tries to return the reduction passed through reduction_name after taking the absolute value of the tensor. It defaults to False .","title":"Arguments"},{"location":"analysis/#returns_15","text":"numpy.ndarray The reduction value of tensor at the given step and worker (if the training job saved data from multiple workers) as a 1x1 numpy array. If this reduction was saved for the tensor during training as part of specification through reduction config, it will be loaded and returned. If the given reduction was not saved then, but the full tensor was saved, the reduction will be computed on the fly and returned. If both the chosen reduction and full tensor are not available, this method raises TensorUnavailableForStep exception.","title":"Returns"},{"location":"analysis/#reduction_values","text":"Get all reduction values saved for the chosen tensor at a particular step. A reduction value is a tensor reduced to a single value through reduction or aggregation operations. Please go through the description of the method reduction_value for more details. trial.tensor(name).reduction_values(step_num, mode=modes.GLOBAL, worker=None)","title":"reduction_values"},{"location":"analysis/#arguments_13","text":"step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode)","title":"Arguments"},{"location":"analysis/#returns_16","text":"dict[(str, bool) -> numpy.ndarray] A dictionary with keys being tuples of the form (reduction_name, abs) to a 1x1 numpy ndarray value. abs here is a boolean that denotes whether the reduction was performed on the absolute value of the tensor or not. Note that this method only returns the reductions which were saved from the training job. It does not compute all known reductions and return them if only the raw tensor was saved.","title":"Returns"},{"location":"analysis/#values","text":"Get the values of the tensor for all steps of a given mode. trial.tensor(name).values(mode=modes.GLOBAL, worker=None)","title":"values"},{"location":"analysis/#arguments_14","text":"mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL worker (str) This parameter is only applicable for distributed training. You can retrieve the value of the tensor from a specific worker by passing the worker name. You can query all the workers seen by the trial with the trial.workers() method. You might also be interested in querying the workers which saved a value for the tensor at a specific step, this is possible with the method: trial.tensor(name).workers(step, mode)","title":"Arguments"},{"location":"analysis/#returns_17","text":"dict[int -> numpy.ndarray] A dictionary with step numbers as keys and numpy arrays representing the value of the tensor as values.","title":"Returns"},{"location":"analysis/#workers_1","text":"Get all the workers for which this tensor was saved at a given step trial.tensor(name).workers(step_num, mode=modes.GLOBAL)","title":"workers"},{"location":"analysis/#arguments_15","text":"step_num (int) The step number whose value is to be returned for the mode passed through the next parameter. mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL","title":"Arguments"},{"location":"analysis/#returns_18","text":"list[str] A list of worker names for which the tensor was saved at the given step.","title":"Returns"},{"location":"analysis/#prev_steps","text":"Get the last n step numbers of a given mode from a given step. trial.tensor(name).prev_steps(step, n, mode=modes.GLOBAL)","title":"prev_steps"},{"location":"analysis/#arguments_16","text":"step (int) The step number whose value is to be returned for the mode passed. n (int) Number of previous steps to return mode (smdebug.modes enum value) The mode applicable for the step number passed above. Defaults to modes.GLOBAL","title":"Arguments"},{"location":"analysis/#returns_19","text":"list[int] A list of size at most n representing the previous steps for the given step and mode. Note that this list can be of size less than n if there were only less than n steps saved before the given step in this trial.","title":"Returns"},{"location":"analysis/#rules","text":"Rules are the medium by which SageMaker Debugger executes a certain piece of code regularly on different steps of a training job. A rule is assigned to a trial and can be invoked at each new step of the trial. It can also access other trials for its evaluation. You can evaluate a rule using tensors from the current step or any step before the current step. Please ensure your logic respects these semantics, else you will get a TensorUnavailableForStep exception as the data would not yet be available for future steps.","title":"Rules"},{"location":"analysis/#built-in-rules","text":"Please refer to the built-in rules that SageMaker provides here .","title":"Built In Rules"},{"location":"analysis/#writing-a-custom-rule","text":"Writing a rule involves implementing the Rule interface . Below, let us look at a simplified version of a VanishingGradient rule.","title":"Writing a custom rule"},{"location":"analysis/#constructor","text":"Creating a rule involves first inheriting from the base Rule class provided by smdebug. For this example rule here, we do not need to look at any other trials, so we set other_trials to None. from smdebug.rules import Rule class VanishingGradientRule(Rule): def __init__(self, base_trial, threshold=0.0000001): super().__init__(base_trial, other_trials=None) self.threshold = float(threshold) Please note that apart from base_trial and other_trials (if required), we require all arguments of the rule constructor to take a string as value. You can parse them to the type that you want from the string. This means if you want to pass a list of strings, you might want to pass them as a comma separated string. This restriction is being enforced so as to let you create and invoke rules from json using Sagemaker's APIs.","title":"Constructor"},{"location":"analysis/#function-to-invoke-at-a-given-step","text":"In this function you can implement the core logic of what you want to do with these tensors. It should return a boolean value True or False , where True means the rule evaluation condition has been met. When you invoke these rules through SageMaker, the rule evaluation ends when the rule evaluation condition is met. SageMaker creates a Cloudwatch event for every rule evaluation job, which can be used to define actions that you might want to take based on the state of the rule. A simplified version of the actual invoke function for VanishingGradientRule is below: def invoke_at_step(self, step): for tensorname in self.base_trial.tensors(collection='gradients'): tensor = self.base_trial.tensor(tensorname) abs_mean = tensor.reduction_value(step, 'mean', abs=True) if abs_mean < self.threshold: return True else: return False That's it, writing a rule is as simple as that.","title":"Function to invoke at a given step"},{"location":"analysis/#invoking-a-rule-through-sagemaker","text":"After you've written your rule, you can ask SageMaker to evaluate the rule against your training job by either using SageMaker Python SDK as estimator = Estimator( ... rules = Rules.custom( name='VGRule', image_uri='864354269164.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', instance_type='ml.t3.medium', # instance type to run the rule evaluation on source='rules/vanishing_gradient_rule.py', # path to the rule source file rule_to_invoke='VanishingGradientRule', # name of the class to invoke in the rule source file volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance collections_to_save=[CollectionConfig(\"gradients\")], # collections to be analyzed by the rule rule_parameters={ \"threshold\": \"20.0\" # this will be used to initialize 'threshold' param in your rule constructor } ) If you're using the SageMaker API directly to evaluate the rule, then you can specify the rule configuration DebugRuleConfigurations in the CreateTrainingJob API request as: \"DebugRuleConfigurations\": [ { \"RuleConfigurationName\": \"VGRule\", \"InstanceType\": \"ml.t3.medium\", \"VolumeSizeInGB\": 30, \"RuleEvaluatorImage\": \"864354269164.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest\", \"RuleParameters\": { \"source_s3_uri\": \"s3://path/to/vanishing_gradient_rule.py\", \"rule_to_invoke\": \"VanishingGradient\", \"threshold\": \"20.0\" } } ]","title":"Invoking a rule through SageMaker"},{"location":"analysis/#invoking-a-rule-outside-of-sagemaker-through-invoke_rule","text":"You might want to invoke the rule locally during development. We provide a function to invoke rules easily. Refer smdebug/rules/rule_invoker.py . The invoke function has the following syntax. It takes a instance of a Rule and invokes it for a series of steps one after the other. from smdebug.rules import invoke_rule from smdebug.trials import create_trial trial = create_trial('s3://smdebug-dev-test/mnist-job/') rule_obj = VanishingGradientRule(trial, threshold=0.0001) invoke_rule(rule_obj, start_step=0, end_step=None)","title":"Invoking a rule outside of SageMaker through invoke_rule"},{"location":"analysis/#arguments_17","text":"rule_obj (Rule) An instance of a subclass of smdebug.rules.Rule that you want to invoke. start_step (int) Global step number to start invoking the rule from. Note that this refers to a global step. This defaults to 0. end_step (int or None) : Global step number to end the invocation of rule before. To clarify, end_step is an exclusive bound. The rule is invoked at end_step . This defaults to None which means run till the end of the job. raise_eval_cond (bool) This parameter controls whether to raise the exception RuleEvaluationConditionMet when raised by the rule, or to catch it and log the message and move to the next step. Defaults to False , which implies that the it catches the exception, logs that the evaluation condition was met for a step and moves on to evaluate the next step.","title":"Arguments"},{"location":"analysis/#exceptions","text":"smdebug is designed to be aware that tensors required to evaluate a rule may not be available at every step. Hence, it raises a few exceptions which allow us to control what happens when a tensor is missing. These are available in the smdebug.exceptions module. You can import them as follows: from smdebug.exceptions import * Here are the exceptions (along with others) and their meaning: TensorUnavailableForStep : This means that the tensor requested is not available for the step. It may have been or will be saved for a different step number. You can check which steps tensor is saved for by trial.tensor('tname').steps() api . Note that this exception implies that the requested tensor will never become available for this step in the future. TensorUnavailable : This means that this tensor has not been saved from the training job. Note that if you have a SaveConfig which saves a certain tensor only after the time you queried for the tensor, you might get a TensorUnavailable exception even if the tensor may become available later for some step. StepUnavailable : This means that the step was not saved from the training job. No tensor will be available for this step. StepNotYetAvailable : This means that the step has not yet been seen from the training job. It may be available in the future if the training is still going on. We automatically load new data as and when it becomes available. This step may either become available in the future, or the exception might change to StepUnavailable . NoMoreData : This will be raised when the training ends. Once you see this, you will know that there will be no more steps and no more tensors saved. RuleEvaluationConditionMet : This is raised when the rule invocation returns True for some step. MissingCollectionFiles : This is raised when no data was saved by the training job. Check that the Hook was configured correctly before starting the training job.","title":"Exceptions"},{"location":"analysis/#utils","text":"","title":"Utils"},{"location":"analysis/#enable-or-disable-refresh-of-tensors-in-a-trial","text":"By default smdebug refreshes tensors each time you try to query the tensor. It looks for whether this tensor is saved for new steps and if so fetches them. If you know the saved data will not change (stopped the machine learning job), or are not interested in the latest data, you can stop the refreshing of tensors as follows: no_refresh takes a trial or a list of trials, which should not be refreshed. Anything executed inside the with no_refresh block will not be refreshed. from smdebug.analysis.utils import no_refresh with no_refresh(trials): pass Similarly if you want to refresh tensors only within a block, you can do: from smdebug.analysis.utils import refresh with refresh(trials): pass During rule invocation smdebug waits till the current step is available and then turns off refresh to ensure that you do not get different results for methods like trial.tensor(name).steps() and run into subtle issues.","title":"Enable or disable refresh of tensors in a trial"},{"location":"api/","text":"Saving Tensors API Glossary Hook Creating a Hook Hook when using SageMaker Python SDK Configuring Hook using SageMaker Python SDK Hook from Python constructor Common Hook API TensorFlow specific Hook API MXNet specific Hook API PyTorch specific Hook API Modes Collection SaveConfig ReductionConfig Glossary The imports assume import smdebug.{tensorflow,pytorch,mxnet,xgboost} as smd . Step : Step means one the work done by the training job for one batch (i.e. forward and backward pass). (An exception is with TensorFlow's Session interface, where a step also includes the initialization session run calls). SageMaker Debugger is designed in terms of steps. When to save data is specified using steps as well as the invocation of Rules is on a step-by-step basis. Hook : The main class to pass as a callback object, or to create callback functions. It keeps track of collections and writes output files at each step. The current hook implementation does not support merging tensors from current job with tensors from previous job(s). Hence ensure that the 'out_dir' does not exist prior to instantiating the 'Hook' object. - hook = smd.Hook(out_dir=\"/tmp/mnist_job\") Mode : One of \"train\", \"eval\", \"predict\", or \"global\". Helpful for segmenting data based on the phase you're in. Defaults to \"global\". - train_mode = smd.modes.TRAIN Collection : A group of tensors. Each collection contains its configuration for what tensors are part of it, and when to save them. - collection = hook.get_collection(\"losses\") SaveConfig : A Python dict specifying how often to save losses and tensors. - save_config = smd.SaveConfig(save_interval=10) ReductionConfig : Allows you to save a reduction, such as 'mean' or 'l1 norm', instead of the full tensor. Reductions are simple floats. - reduction_config = smd.ReductionConfig(reductions=['min', 'max', 'mean'], norms=['l1']) Trial : The main interface to use when analyzing a completed training job. Access collections and tensors. See trials documentation . - trial = smd.create_trial(out_dir=\"/tmp/mnist_job\") Rule : A condition to monitor the saved data for. It can trigger an exception when the condition is met, for example a vanishing gradient. See rules documentation . Hook Creating a Hook Note that when using Zero Script Change supported containers in SageMaker, you generally do not need to create your hook object except for some advanced use cases where you need access to the hook. HookClass or hook_class below will be Hook for PyTorch, MXNet, and XGBoost. It will be one of KerasHook , SessionHook or EstimatorHook for TensorFlow. The framework in smd import below refers to one of tensorflow , mxnet , pytorch or xgboost . Hook when using SageMaker Python SDK If you create a SageMaker job and specify the hook configuration in the SageMaker Estimator API as described in AWS Docs , a JSON file containing the hook configuration will be automatically written to the training container. In such a case, you can create a hook from that configuration file by calling import smdebug.{framework} as smd hook = smd.{hook_class}.create_from_json_file() with no arguments and then use the hook Python API in your script. Configuring Hook using SageMaker Python SDK Parameters to the Hook are passed as below when using the SageMaker Python SDK. from sagemaker.debugger import DebuggerHookConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', hook_parameters={ \"parameter\": \"value\" }) The parameters can be one of the following. The meaning of these parameters will be clear as you review the sections of documentation below. Note that all parameters below have to be strings. So for any parameter which accepts a list (such as save_steps, reductions, include_regex), the value needs to be given as strings separated by a comma between them. dry_run save_all include_workers include_regex reductions save_raw_tensor save_interval save_steps start_step end_step train.save_interval train.save_steps train.start_step train.end_step eval.save_interval eval.save_steps eval.start_step eval.end_step predict.save_interval predict.save_steps predict.start_step predict.end_step global.save_interval global.save_steps global.start_step global.end_step Hook from Python constructor See the framework-specific pages for more details. HookClass below can be one of KerasHook , SessionHook , EstimatorHook for TensorFlow, or is just Hook for MXNet, Pytorch and XGBoost. hook = HookClass( out_dir, export_tensorboard = False, tensorboard_dir = None, dry_run = False, reduction_config = None, save_config = None, include_regex = None, include_collections = None, save_all = False, include_workers=\"one\" ) Arguments out_dir (str): Path where to save tensors and metadata. This is a required argument. Please ensure that the 'out_dir' does not exist. export_tensorboard (bool): Whether to export TensorBoard summaries (distributions and histograms for tensors saved, and scalar summaries for scalars saved). Defaults to False . Note that when running on SageMaker this parameter will be ignored. You will need to use the TensorBoardOutputConfig section in API to enable TensorBoard summaries. Refer SageMaker page for an example. tensorboard_dir (str): Path where to save TensorBoard artifacts. If this is not passed and export_tensorboard is True, then TensorBoard artifacts are saved in out_dir/tensorboard . Note that when running on SageMaker this parameter will be ignored. You will need to use the TensorBoardOutputConfig section in API to enable TensorBoard summaries. Refer SageMaker page for an example. dry_run (bool): If true, don't write any files reduction_config : ( ReductionConfig object) Specifies the reductions to be applied as default for tensors saved. A collection can have its own ReductionConfig object which overrides this for the tensors which belong to that collection. save_config : ( SaveConfig object) Specifies when to save tensors. A collection can have its own SaveConfig object which overrides this for the tensors which belong to that collection. include_regex (list[str]): list of regex patterns which specify the tensors to save. Tensors whose names match these patterns will be saved include_collections (list[str]): List of which collections to save specified by name save_all (bool): Saves all tensors and collections. Increases the amount of disk space used, and can reduce the performance of the training job significantly, depending on the size of the model. include_workers (str): Used for distributed training. It can take the values one or all . one means only the tensors from one chosen worker will be saved. This is the default behavior. all means tensors from all workers will be saved. Common Hook API These methods are common for all hooks in any framework. Note that smd import below translates to import smdebug.{framework} as smd . Method Arguments Behavior add_collection(collection) collection (smd.Collection) Takes a Collection object and adds it to the CollectionManager that the Hook holds. Note that you should only pass in a Collection object for the same framework as the hook get_collection(name) name (str) Returns collection identified by the given name get_collections() - Returns all collection objects held by the hook set_mode(mode) value of the enum smd.modes Sets mode of the job, can be one of smd.modes.TRAIN , smd.modes.EVAL , smd.modes.PREDICT or smd.modes.GLOBAL . Refer Modes for more on that. create_from_json_file( json_file_path=None) json_file_path (str) Takes the path of a file which holds the json configuration of the hook, and creates hook from that configuration. This is an optional parameter. If this is not passed it tries to get the file path from the value of the environment variable SMDEBUG_CONFIG_FILE_PATH and defaults to /opt/ml/input/config/debughookconfig.json . When training on SageMaker you do not have to specify any path because this is the default path that SageMaker writes the hook configuration to. close() - Closes all files that are currently open by the hook save_scalar( name, value, sm_metric=False) name (str) value (float) sm_metric (bool) Saves a scalar value by the given name. Passing sm_metric=True flag also makes this scalar available as a SageMaker Metric to show up in SageMaker Studio. Note that when sm_metric is False, this scalar always resides only in your AWS account, but setting it to True saves the scalar also on AWS servers. The default value of sm_metric for this method is False. TensorFlow specific Hook API Note that there are three types of Hooks in TensorFlow: SessionHook, EstimatorHook and KerasHook based on the TensorFlow interface being used for training. This page shows examples of each of these. Method Arguments Returns Behavior wrap_optimizer(optimizer) optimizer (tf.train.Optimizer or tf.keras.Optimizer) Returns the same optimizer object passed with a couple of identifying markers to help smdebug . This returned optimizer should be used for training. When not using Zero Script Change environments, calling this method on your optimizer is necessary for SageMaker Debugger to identify and save gradient tensors. Note that this method returns the same optimizer object passed and does not change your optimization logic. If the hook is of type KerasHook , you can pass in either an object of type tf.train.Optimizer or tf.keras.Optimizer . If the hook is of type SessionHook or EstimatorHook , the optimizer can only be of type tf.train.Optimizer . This new add_to_collection( collection_name, variable) collection_name (str) : name of the collection to add to. variable parameter to pass to the collection's add method. None Calls the add method of a collection object. See this section for more. APIs specific to training scripts using TF 2.x GradientTape ( Example ): Method Arguments Returns Behavior wrap_tape(tape) tape (tensorflow.python.eager.backprop.GradientTape) Returns a tape object with three identifying markers to help smdebug . This returned tape should be used for training. When not using Zero Script Change environments, calling this method on your tape is necessary for SageMaker Debugger to identify and save gradient tensors. Note that this method returns the same tape object passed. record_tensor_value( tensor_name, tensor_value) tensor_name (str) : name of the tensor to save. tensor_value EagerTensor to save. None Manually save metrics tensors while using TF 2.x GradientTape. MXNet specific Hook API Method Arguments Behavior register_block(block) block (mx.gluon.Block) Calling this method applies the hook to the Gluon block representing the model, so SageMaker Debugger gets called by MXNet and can save the tensors required. PyTorch specific Hook API Method Arguments Behavior register_module(module) module (torch.nn.Module) Calling this method applies the hook to the Torch Module representing the model, so SageMaker Debugger gets called by PyTorch and can save the tensors required. register_loss(loss_module) loss_module (torch.nn.modules.loss._Loss) Calling this method applies the hook to the Torch Module representing the loss, so SageMaker Debugger can save losses Modes Used to signify which part of training you're in, similar to Keras modes. GLOBAL mode is used as a default when no mode was set. Choose from smdebug.modes.TRAIN smdebug.modes.EVAL smdebug.modes.PREDICT smdebug.modes.GLOBAL The modes enum is also available under the alias smdebug.{framework}.modes . Collection The construct of a Collection groups tensors together. A Collection is identified by a string representing the name of the collection. It can be used to group tensors of a particular kind such as \"losses\", \"weights\", \"biases\", or \"gradients\". A Collection has its own list of tensors specified by include regex patterns, and other parameters determining how these tensors should be saved and when. Using collections enables you to save different types of tensors at different frequencies and in different forms. These collections are then also available during analysis so you can query a group of tensors at once. There are a number of built-in collections that SageMaker Debugger manages by default. This means that the library takes care of identifying what tensors should be saved as part of that collection. You can also define custom collections, to do which there are couple of different ways. You can specify which of these collections to save in the hook's include_collections parameter, or through the collection_configs parameter to the DebuggerHookConfig in the SageMaker Python SDK. Built in Collections Below is a comprehensive list of the built-in collections that are managed by SageMaker Debugger. The Hook identifes the tensors that should be saved as part of that collection for that framework and saves them if they were requested. The names of these collections are all lower case strings. Name Supported by frameworks/hooks Description all all Matches all tensors default all It's a default collection created, which matches the regex patterns passed as include_regex to the Hook weights TensorFlow, PyTorch, MXNet Matches all weights of the model biases TensorFlow, PyTorch, MXNet Matches all biases of the model gradients TensorFlow, PyTorch, MXNet Matches all gradients of the model. In TensorFlow when not using Zero Script Change environments, must use hook.wrap_optimizer() . losses TensorFlow, PyTorch, MXNet Saves the loss for the model metrics TensorFlow's KerasHook, XGBoost For KerasHook, saves the metrics computed by Keras for the model. For XGBoost, the evaluation metrics computed by the algorithm. outputs TensorFlow's KerasHook Matches the outputs of the model sm_metrics TensorFlow You can add scalars that you want to show up in SageMaker Metrics to this collection. SageMaker Debugger will save these scalars both to the out_dir of the hook, as well as to SageMaker Metric. Note that the scalars passed here will be saved on AWS servers outside of your AWS account. optimizer_variables TensorFlow's KerasHook Matches all optimizer variables, currently only supported in Keras. hyperparameters XGBoost Booster paramameters predictions XGBoost Predictions on validation set (if provided) labels XGBoost Labels on validation set (if provided) feature_importance XGBoost Feature importance given by get_score() full_shap XGBoost A matrix of (nsmaple, nfeatures + 1) with each record indicating the feature contributions ( SHAP values ) for that prediction. Computed on training data with predict() average_shap XGBoost The sum of SHAP value magnitudes over all samples. Represents the impact each feature has on the model output. trees XGBoost Boosted tree model given by trees_to_dataframe() Default collections saved The following collections are saved regardless of the hook configuration. Framework Default collections saved TensorFlow METRICS, LOSSES, SM_METRICS PyTorch LOSSES MXNet LOSSES XGBoost METRICS If for some reason, you want to disable the saving of these collections, you can do so by setting end_step to 0 in the collection's SaveConfig. When using the SageMaker Python SDK this would look like python from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig(name=\"metrics\", parameters={\"end_step\": 0}) ] ) When configuring the Collection in your Python script, it would be as follows: python hook.get_collection(\"metrics\").save_config.end_step = 0 Creating or retrieving a Collection Function Behavior hook.get_collection(collection_name) Returns the collection with the given name. Creates the collection with default configuration if it doesn't already exist. A new collection created by default does not match any tensor and is configured to save histograms and distributions along with the tensor if tensorboard support is enabled, and uses the reduction configuration and save configuration passed to the hook. Properties of a Collection Property Description tensor_names Get or set list of tensor names as strings include_regex Get or set list of regexes to include. Tensors whose names match these regex patterns will be included in the collection reduction_config Get or set the ReductionConfig object to be used for tensors part of this collection save_config Get or set the SaveConfig object to be used for tensors part of this collection save_histogram Get or set the boolean flag which determines whether to write histograms to enable histograms and distributions in TensorBoard, for tensors part of this collection. Only applicable if TensorBoard support is enabled. Methods on a Collection Method Behavior coll.include(regex) Takes a regex string or a list of regex strings to match tensors to include in the collection. coll.add(tensor) (TensorFlow only) Takes an instance or list or set of tf.Tensor/tf.Variable/tf.MirroredVariable/tf.Operation to add to the collection. coll.add_keras_layer(layer, inputs=False, outputs=True) (tf.keras only) Takes an instance of a tf.keras layer and logs input/output tensors for that module. By default, only outputs are saved. coll.add_module_tensors(module, inputs=False, outputs=True) (PyTorch only) Takes an instance of a PyTorch module and logs input/output tensors for that module. By default, only outputs are saved. coll.add_block_tensors(block, inputs=False, outputs=True) (MXNet only) Takes an instance of a Gluon block,and logs input/output tensors for that module. By default, only outputs are saved. Configuring Collection using SageMaker Python SDK Parameters to configure Collection are passed as below when using the SageMaker Python SDK. from sagemaker.debugger import CollectionConfig coll_config = CollectionConfig( name=\"weights\", parameters={ \"parameter\": \"value\" }) The parameters can be one of the following. The meaning of these parameters will be clear as you review the sections of documentation below. Note that all parameters below have to be strings. So any parameter which accepts a list (such as save_steps, reductions, include_regex), needs to be given as strings separated by a comma between them. include_regex save_histogram reductions save_raw_tensor save_interval save_steps start_step end_step train.save_interval train.save_steps train.start_step train.end_step eval.save_interval eval.save_steps eval.start_step eval.end_step predict.save_interval predict.save_steps predict.start_step predict.end_step global.save_interval global.save_steps global.start_step global.end_step SaveConfig The SaveConfig class customizes the frequency of saving tensors. The hook takes a SaveConfig object which is applied as default to all tensors included. A collection can also have a SaveConfig object which is applied to the collection's tensors. You can also choose to have different configuration for when to save tensors based on the mode of the job. This class is available in the following namespaces smdebug and smdebug.{framework} . import smdebug as smd save_config = smd.SaveConfig( mode_save_configs = None, save_interval = 100, start_step = 0, end_step = None, save_steps = None, ) Arguments mode_save_configs (dict): Used for advanced cases; see details below. save_interval (int): How often, in steps, to save tensors. Defaults to 500. A step is saved if step % save_interval == 0 start_step (int): When to start saving tensors. end_step (int): When to stop saving tensors, exclusive. save_steps (list[int]): Specific steps to save tensors at. Union with save_interval. Examples SaveConfig() will save at steps 0, 500, ... SaveConfig(save_interval=1) will save at steps 0, 1, ... SaveConfig(save_interval=100, end_step=200) will save at steps 0, 100 SaveConfig(save_interval=100, end_step=201) will save at steps 0, 100, 200 SaveConfig(save_interval=100, start_step=150) will save at steps 200, 300, ... SaveConfig(save_steps=[3, 7]) will save at steps 0, 3, 7, 500, ... Specifying different configuration based on mode There is also a more advanced use case, where you specify a different SaveConfig for each mode. It is best understood through an example: import smdebug as smd smd.SaveConfig(mode_save_configs={ smd.modes.TRAIN: smd.SaveConfigMode(save_interval=1), smd.modes.EVAL: smd.SaveConfigMode(save_interval=2), smd.modes.PREDICT: smd.SaveConfigMode(save_interval=3), smd.modes.GLOBAL: smd.SaveConfigMode(save_interval=4) }) Essentially, create a dictionary mapping modes to SaveConfigMode objects. The SaveConfigMode objects take the same four parameters (save_interval, start_step, end_step, save_steps) as the main object. Any mode not specified will default to the default configuration. If a mode is provided but not all params are specified, we use the default values for non-specified parameters. Configuration using SageMaker Python SDK Refer Configuring Hook using SageMaker Python SDK and Configuring Collection using SageMaker Python SDK ReductionConfig ReductionConfig allows the saving of certain reductions of tensors instead of saving the full tensor. The motivation here is to reduce the amount of data saved, and increase the speed in cases where you don't need the full tensor. The reduction operations which are computed in the training process and then saved. During analysis, these are available as reductions of the original tensor. Please note that using reduction config means that you will not have the full tensor available during analysis, so this can restrict what you can do with the tensor saved. You can choose to also save the raw tensor along with the reductions if you so desire. The hook takes a ReductionConfig object which is applied as default to all tensors included. A collection can also have its own ReductionConfig object which is applied to the tensors belonging to that collection. import smdebug as smd reduction_config = smd.ReductionConfig( reductions = None, abs_reductions = None, norms = None, abs_norms = None, save_raw_tensor = False, ) Arguments reductions (list[str]): Takes names of reductions, choosing from \"min\", \"max\", \"median\", \"mean\", \"std\", \"variance\", \"sum\", \"prod\" abs_reductions (list[str]): Same as reductions, except the reduction will be computed on the absolute value of the tensor norms (list[str]): Takes names of norms to compute, choosing from \"l1\", \"l2\" abs_norms (list[str]): Same as norms, except the norm will be computed on the absolute value of the tensor save_raw_tensor (bool): Saves the tensor directly, in addition to other desired reductions For example, ReductionConfig(reductions=['std', 'variance'], abs_reductions=['mean'], norms=['l1']) will save the standard deviation and variance, the mean of the absolute value, and the l1 norm. Configuration using SageMaker Python SDK The reductions are passed as part of the \"reductions\" parameter to HookParameters or Collection Parameters. Refer Configuring Hook using SageMaker Python SDK and Configuring Collection using SageMaker Python SDK for more on that. The parameter \"reductions\" can take a comma separated string consisting of the following values: min max median mean std variance sum prod l1 l2 abs_min abs_max abs_median abs_mean abs_std abs_variance abs_sum abs_prod abs_l1 abs_l2 Frameworks For details on what's supported for different framework, go here: * TensorFlow * PyTorch * MXNet * XGBoost","title":"Saving Tensors API"},{"location":"api/#saving-tensors-api","text":"Glossary Hook Creating a Hook Hook when using SageMaker Python SDK Configuring Hook using SageMaker Python SDK Hook from Python constructor Common Hook API TensorFlow specific Hook API MXNet specific Hook API PyTorch specific Hook API Modes Collection SaveConfig ReductionConfig","title":"Saving Tensors API"},{"location":"api/#glossary","text":"The imports assume import smdebug.{tensorflow,pytorch,mxnet,xgboost} as smd . Step : Step means one the work done by the training job for one batch (i.e. forward and backward pass). (An exception is with TensorFlow's Session interface, where a step also includes the initialization session run calls). SageMaker Debugger is designed in terms of steps. When to save data is specified using steps as well as the invocation of Rules is on a step-by-step basis. Hook : The main class to pass as a callback object, or to create callback functions. It keeps track of collections and writes output files at each step. The current hook implementation does not support merging tensors from current job with tensors from previous job(s). Hence ensure that the 'out_dir' does not exist prior to instantiating the 'Hook' object. - hook = smd.Hook(out_dir=\"/tmp/mnist_job\") Mode : One of \"train\", \"eval\", \"predict\", or \"global\". Helpful for segmenting data based on the phase you're in. Defaults to \"global\". - train_mode = smd.modes.TRAIN Collection : A group of tensors. Each collection contains its configuration for what tensors are part of it, and when to save them. - collection = hook.get_collection(\"losses\") SaveConfig : A Python dict specifying how often to save losses and tensors. - save_config = smd.SaveConfig(save_interval=10) ReductionConfig : Allows you to save a reduction, such as 'mean' or 'l1 norm', instead of the full tensor. Reductions are simple floats. - reduction_config = smd.ReductionConfig(reductions=['min', 'max', 'mean'], norms=['l1']) Trial : The main interface to use when analyzing a completed training job. Access collections and tensors. See trials documentation . - trial = smd.create_trial(out_dir=\"/tmp/mnist_job\") Rule : A condition to monitor the saved data for. It can trigger an exception when the condition is met, for example a vanishing gradient. See rules documentation .","title":"Glossary"},{"location":"api/#hook","text":"","title":"Hook"},{"location":"api/#creating-a-hook","text":"Note that when using Zero Script Change supported containers in SageMaker, you generally do not need to create your hook object except for some advanced use cases where you need access to the hook. HookClass or hook_class below will be Hook for PyTorch, MXNet, and XGBoost. It will be one of KerasHook , SessionHook or EstimatorHook for TensorFlow. The framework in smd import below refers to one of tensorflow , mxnet , pytorch or xgboost .","title":"Creating a Hook"},{"location":"api/#hook-when-using-sagemaker-python-sdk","text":"If you create a SageMaker job and specify the hook configuration in the SageMaker Estimator API as described in AWS Docs , a JSON file containing the hook configuration will be automatically written to the training container. In such a case, you can create a hook from that configuration file by calling import smdebug.{framework} as smd hook = smd.{hook_class}.create_from_json_file() with no arguments and then use the hook Python API in your script.","title":"Hook when using SageMaker Python SDK"},{"location":"api/#configuring-hook-using-sagemaker-python-sdk","text":"Parameters to the Hook are passed as below when using the SageMaker Python SDK. from sagemaker.debugger import DebuggerHookConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', hook_parameters={ \"parameter\": \"value\" }) The parameters can be one of the following. The meaning of these parameters will be clear as you review the sections of documentation below. Note that all parameters below have to be strings. So for any parameter which accepts a list (such as save_steps, reductions, include_regex), the value needs to be given as strings separated by a comma between them. dry_run save_all include_workers include_regex reductions save_raw_tensor save_interval save_steps start_step end_step train.save_interval train.save_steps train.start_step train.end_step eval.save_interval eval.save_steps eval.start_step eval.end_step predict.save_interval predict.save_steps predict.start_step predict.end_step global.save_interval global.save_steps global.start_step global.end_step","title":"Configuring Hook using SageMaker Python SDK"},{"location":"api/#hook-from-python-constructor","text":"See the framework-specific pages for more details. HookClass below can be one of KerasHook , SessionHook , EstimatorHook for TensorFlow, or is just Hook for MXNet, Pytorch and XGBoost. hook = HookClass( out_dir, export_tensorboard = False, tensorboard_dir = None, dry_run = False, reduction_config = None, save_config = None, include_regex = None, include_collections = None, save_all = False, include_workers=\"one\" )","title":"Hook from Python constructor"},{"location":"api/#arguments","text":"out_dir (str): Path where to save tensors and metadata. This is a required argument. Please ensure that the 'out_dir' does not exist. export_tensorboard (bool): Whether to export TensorBoard summaries (distributions and histograms for tensors saved, and scalar summaries for scalars saved). Defaults to False . Note that when running on SageMaker this parameter will be ignored. You will need to use the TensorBoardOutputConfig section in API to enable TensorBoard summaries. Refer SageMaker page for an example. tensorboard_dir (str): Path where to save TensorBoard artifacts. If this is not passed and export_tensorboard is True, then TensorBoard artifacts are saved in out_dir/tensorboard . Note that when running on SageMaker this parameter will be ignored. You will need to use the TensorBoardOutputConfig section in API to enable TensorBoard summaries. Refer SageMaker page for an example. dry_run (bool): If true, don't write any files reduction_config : ( ReductionConfig object) Specifies the reductions to be applied as default for tensors saved. A collection can have its own ReductionConfig object which overrides this for the tensors which belong to that collection. save_config : ( SaveConfig object) Specifies when to save tensors. A collection can have its own SaveConfig object which overrides this for the tensors which belong to that collection. include_regex (list[str]): list of regex patterns which specify the tensors to save. Tensors whose names match these patterns will be saved include_collections (list[str]): List of which collections to save specified by name save_all (bool): Saves all tensors and collections. Increases the amount of disk space used, and can reduce the performance of the training job significantly, depending on the size of the model. include_workers (str): Used for distributed training. It can take the values one or all . one means only the tensors from one chosen worker will be saved. This is the default behavior. all means tensors from all workers will be saved.","title":"Arguments"},{"location":"api/#common-hook-api","text":"These methods are common for all hooks in any framework. Note that smd import below translates to import smdebug.{framework} as smd . Method Arguments Behavior add_collection(collection) collection (smd.Collection) Takes a Collection object and adds it to the CollectionManager that the Hook holds. Note that you should only pass in a Collection object for the same framework as the hook get_collection(name) name (str) Returns collection identified by the given name get_collections() - Returns all collection objects held by the hook set_mode(mode) value of the enum smd.modes Sets mode of the job, can be one of smd.modes.TRAIN , smd.modes.EVAL , smd.modes.PREDICT or smd.modes.GLOBAL . Refer Modes for more on that. create_from_json_file( json_file_path=None) json_file_path (str) Takes the path of a file which holds the json configuration of the hook, and creates hook from that configuration. This is an optional parameter. If this is not passed it tries to get the file path from the value of the environment variable SMDEBUG_CONFIG_FILE_PATH and defaults to /opt/ml/input/config/debughookconfig.json . When training on SageMaker you do not have to specify any path because this is the default path that SageMaker writes the hook configuration to. close() - Closes all files that are currently open by the hook save_scalar( name, value, sm_metric=False) name (str) value (float) sm_metric (bool) Saves a scalar value by the given name. Passing sm_metric=True flag also makes this scalar available as a SageMaker Metric to show up in SageMaker Studio. Note that when sm_metric is False, this scalar always resides only in your AWS account, but setting it to True saves the scalar also on AWS servers. The default value of sm_metric for this method is False.","title":"Common Hook API"},{"location":"api/#tensorflow-specific-hook-api","text":"Note that there are three types of Hooks in TensorFlow: SessionHook, EstimatorHook and KerasHook based on the TensorFlow interface being used for training. This page shows examples of each of these. Method Arguments Returns Behavior wrap_optimizer(optimizer) optimizer (tf.train.Optimizer or tf.keras.Optimizer) Returns the same optimizer object passed with a couple of identifying markers to help smdebug . This returned optimizer should be used for training. When not using Zero Script Change environments, calling this method on your optimizer is necessary for SageMaker Debugger to identify and save gradient tensors. Note that this method returns the same optimizer object passed and does not change your optimization logic. If the hook is of type KerasHook , you can pass in either an object of type tf.train.Optimizer or tf.keras.Optimizer . If the hook is of type SessionHook or EstimatorHook , the optimizer can only be of type tf.train.Optimizer . This new add_to_collection( collection_name, variable) collection_name (str) : name of the collection to add to. variable parameter to pass to the collection's add method. None Calls the add method of a collection object. See this section for more. APIs specific to training scripts using TF 2.x GradientTape ( Example ): Method Arguments Returns Behavior wrap_tape(tape) tape (tensorflow.python.eager.backprop.GradientTape) Returns a tape object with three identifying markers to help smdebug . This returned tape should be used for training. When not using Zero Script Change environments, calling this method on your tape is necessary for SageMaker Debugger to identify and save gradient tensors. Note that this method returns the same tape object passed. record_tensor_value( tensor_name, tensor_value) tensor_name (str) : name of the tensor to save. tensor_value EagerTensor to save. None Manually save metrics tensors while using TF 2.x GradientTape.","title":"TensorFlow specific Hook API"},{"location":"api/#mxnet-specific-hook-api","text":"Method Arguments Behavior register_block(block) block (mx.gluon.Block) Calling this method applies the hook to the Gluon block representing the model, so SageMaker Debugger gets called by MXNet and can save the tensors required.","title":"MXNet specific Hook API"},{"location":"api/#pytorch-specific-hook-api","text":"Method Arguments Behavior register_module(module) module (torch.nn.Module) Calling this method applies the hook to the Torch Module representing the model, so SageMaker Debugger gets called by PyTorch and can save the tensors required. register_loss(loss_module) loss_module (torch.nn.modules.loss._Loss) Calling this method applies the hook to the Torch Module representing the loss, so SageMaker Debugger can save losses","title":"PyTorch specific Hook API"},{"location":"api/#modes","text":"Used to signify which part of training you're in, similar to Keras modes. GLOBAL mode is used as a default when no mode was set. Choose from smdebug.modes.TRAIN smdebug.modes.EVAL smdebug.modes.PREDICT smdebug.modes.GLOBAL The modes enum is also available under the alias smdebug.{framework}.modes .","title":"Modes"},{"location":"api/#collection","text":"The construct of a Collection groups tensors together. A Collection is identified by a string representing the name of the collection. It can be used to group tensors of a particular kind such as \"losses\", \"weights\", \"biases\", or \"gradients\". A Collection has its own list of tensors specified by include regex patterns, and other parameters determining how these tensors should be saved and when. Using collections enables you to save different types of tensors at different frequencies and in different forms. These collections are then also available during analysis so you can query a group of tensors at once. There are a number of built-in collections that SageMaker Debugger manages by default. This means that the library takes care of identifying what tensors should be saved as part of that collection. You can also define custom collections, to do which there are couple of different ways. You can specify which of these collections to save in the hook's include_collections parameter, or through the collection_configs parameter to the DebuggerHookConfig in the SageMaker Python SDK.","title":"Collection"},{"location":"api/#built-in-collections","text":"Below is a comprehensive list of the built-in collections that are managed by SageMaker Debugger. The Hook identifes the tensors that should be saved as part of that collection for that framework and saves them if they were requested. The names of these collections are all lower case strings. Name Supported by frameworks/hooks Description all all Matches all tensors default all It's a default collection created, which matches the regex patterns passed as include_regex to the Hook weights TensorFlow, PyTorch, MXNet Matches all weights of the model biases TensorFlow, PyTorch, MXNet Matches all biases of the model gradients TensorFlow, PyTorch, MXNet Matches all gradients of the model. In TensorFlow when not using Zero Script Change environments, must use hook.wrap_optimizer() . losses TensorFlow, PyTorch, MXNet Saves the loss for the model metrics TensorFlow's KerasHook, XGBoost For KerasHook, saves the metrics computed by Keras for the model. For XGBoost, the evaluation metrics computed by the algorithm. outputs TensorFlow's KerasHook Matches the outputs of the model sm_metrics TensorFlow You can add scalars that you want to show up in SageMaker Metrics to this collection. SageMaker Debugger will save these scalars both to the out_dir of the hook, as well as to SageMaker Metric. Note that the scalars passed here will be saved on AWS servers outside of your AWS account. optimizer_variables TensorFlow's KerasHook Matches all optimizer variables, currently only supported in Keras. hyperparameters XGBoost Booster paramameters predictions XGBoost Predictions on validation set (if provided) labels XGBoost Labels on validation set (if provided) feature_importance XGBoost Feature importance given by get_score() full_shap XGBoost A matrix of (nsmaple, nfeatures + 1) with each record indicating the feature contributions ( SHAP values ) for that prediction. Computed on training data with predict() average_shap XGBoost The sum of SHAP value magnitudes over all samples. Represents the impact each feature has on the model output. trees XGBoost Boosted tree model given by trees_to_dataframe()","title":"Built in Collections"},{"location":"api/#default-collections-saved","text":"The following collections are saved regardless of the hook configuration. Framework Default collections saved TensorFlow METRICS, LOSSES, SM_METRICS PyTorch LOSSES MXNet LOSSES XGBoost METRICS If for some reason, you want to disable the saving of these collections, you can do so by setting end_step to 0 in the collection's SaveConfig. When using the SageMaker Python SDK this would look like python from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig(name=\"metrics\", parameters={\"end_step\": 0}) ] ) When configuring the Collection in your Python script, it would be as follows: python hook.get_collection(\"metrics\").save_config.end_step = 0","title":"Default collections saved"},{"location":"api/#creating-or-retrieving-a-collection","text":"Function Behavior hook.get_collection(collection_name) Returns the collection with the given name. Creates the collection with default configuration if it doesn't already exist. A new collection created by default does not match any tensor and is configured to save histograms and distributions along with the tensor if tensorboard support is enabled, and uses the reduction configuration and save configuration passed to the hook.","title":"Creating or retrieving a Collection"},{"location":"api/#properties-of-a-collection","text":"Property Description tensor_names Get or set list of tensor names as strings include_regex Get or set list of regexes to include. Tensors whose names match these regex patterns will be included in the collection reduction_config Get or set the ReductionConfig object to be used for tensors part of this collection save_config Get or set the SaveConfig object to be used for tensors part of this collection save_histogram Get or set the boolean flag which determines whether to write histograms to enable histograms and distributions in TensorBoard, for tensors part of this collection. Only applicable if TensorBoard support is enabled.","title":"Properties of a Collection"},{"location":"api/#methods-on-a-collection","text":"Method Behavior coll.include(regex) Takes a regex string or a list of regex strings to match tensors to include in the collection. coll.add(tensor) (TensorFlow only) Takes an instance or list or set of tf.Tensor/tf.Variable/tf.MirroredVariable/tf.Operation to add to the collection. coll.add_keras_layer(layer, inputs=False, outputs=True) (tf.keras only) Takes an instance of a tf.keras layer and logs input/output tensors for that module. By default, only outputs are saved. coll.add_module_tensors(module, inputs=False, outputs=True) (PyTorch only) Takes an instance of a PyTorch module and logs input/output tensors for that module. By default, only outputs are saved. coll.add_block_tensors(block, inputs=False, outputs=True) (MXNet only) Takes an instance of a Gluon block,and logs input/output tensors for that module. By default, only outputs are saved.","title":"Methods on a Collection"},{"location":"api/#configuring-collection-using-sagemaker-python-sdk","text":"Parameters to configure Collection are passed as below when using the SageMaker Python SDK. from sagemaker.debugger import CollectionConfig coll_config = CollectionConfig( name=\"weights\", parameters={ \"parameter\": \"value\" }) The parameters can be one of the following. The meaning of these parameters will be clear as you review the sections of documentation below. Note that all parameters below have to be strings. So any parameter which accepts a list (such as save_steps, reductions, include_regex), needs to be given as strings separated by a comma between them. include_regex save_histogram reductions save_raw_tensor save_interval save_steps start_step end_step train.save_interval train.save_steps train.start_step train.end_step eval.save_interval eval.save_steps eval.start_step eval.end_step predict.save_interval predict.save_steps predict.start_step predict.end_step global.save_interval global.save_steps global.start_step global.end_step","title":"Configuring Collection using SageMaker Python SDK"},{"location":"api/#saveconfig","text":"The SaveConfig class customizes the frequency of saving tensors. The hook takes a SaveConfig object which is applied as default to all tensors included. A collection can also have a SaveConfig object which is applied to the collection's tensors. You can also choose to have different configuration for when to save tensors based on the mode of the job. This class is available in the following namespaces smdebug and smdebug.{framework} . import smdebug as smd save_config = smd.SaveConfig( mode_save_configs = None, save_interval = 100, start_step = 0, end_step = None, save_steps = None, )","title":"SaveConfig"},{"location":"api/#arguments_1","text":"mode_save_configs (dict): Used for advanced cases; see details below. save_interval (int): How often, in steps, to save tensors. Defaults to 500. A step is saved if step % save_interval == 0 start_step (int): When to start saving tensors. end_step (int): When to stop saving tensors, exclusive. save_steps (list[int]): Specific steps to save tensors at. Union with save_interval.","title":"Arguments"},{"location":"api/#examples","text":"SaveConfig() will save at steps 0, 500, ... SaveConfig(save_interval=1) will save at steps 0, 1, ... SaveConfig(save_interval=100, end_step=200) will save at steps 0, 100 SaveConfig(save_interval=100, end_step=201) will save at steps 0, 100, 200 SaveConfig(save_interval=100, start_step=150) will save at steps 200, 300, ... SaveConfig(save_steps=[3, 7]) will save at steps 0, 3, 7, 500, ...","title":"Examples"},{"location":"api/#specifying-different-configuration-based-on-mode","text":"There is also a more advanced use case, where you specify a different SaveConfig for each mode. It is best understood through an example: import smdebug as smd smd.SaveConfig(mode_save_configs={ smd.modes.TRAIN: smd.SaveConfigMode(save_interval=1), smd.modes.EVAL: smd.SaveConfigMode(save_interval=2), smd.modes.PREDICT: smd.SaveConfigMode(save_interval=3), smd.modes.GLOBAL: smd.SaveConfigMode(save_interval=4) }) Essentially, create a dictionary mapping modes to SaveConfigMode objects. The SaveConfigMode objects take the same four parameters (save_interval, start_step, end_step, save_steps) as the main object. Any mode not specified will default to the default configuration. If a mode is provided but not all params are specified, we use the default values for non-specified parameters.","title":"Specifying different configuration based on mode"},{"location":"api/#configuration-using-sagemaker-python-sdk","text":"Refer Configuring Hook using SageMaker Python SDK and Configuring Collection using SageMaker Python SDK","title":"Configuration using SageMaker Python SDK"},{"location":"api/#reductionconfig","text":"ReductionConfig allows the saving of certain reductions of tensors instead of saving the full tensor. The motivation here is to reduce the amount of data saved, and increase the speed in cases where you don't need the full tensor. The reduction operations which are computed in the training process and then saved. During analysis, these are available as reductions of the original tensor. Please note that using reduction config means that you will not have the full tensor available during analysis, so this can restrict what you can do with the tensor saved. You can choose to also save the raw tensor along with the reductions if you so desire. The hook takes a ReductionConfig object which is applied as default to all tensors included. A collection can also have its own ReductionConfig object which is applied to the tensors belonging to that collection. import smdebug as smd reduction_config = smd.ReductionConfig( reductions = None, abs_reductions = None, norms = None, abs_norms = None, save_raw_tensor = False, )","title":"ReductionConfig"},{"location":"api/#arguments_2","text":"reductions (list[str]): Takes names of reductions, choosing from \"min\", \"max\", \"median\", \"mean\", \"std\", \"variance\", \"sum\", \"prod\" abs_reductions (list[str]): Same as reductions, except the reduction will be computed on the absolute value of the tensor norms (list[str]): Takes names of norms to compute, choosing from \"l1\", \"l2\" abs_norms (list[str]): Same as norms, except the norm will be computed on the absolute value of the tensor save_raw_tensor (bool): Saves the tensor directly, in addition to other desired reductions For example, ReductionConfig(reductions=['std', 'variance'], abs_reductions=['mean'], norms=['l1']) will save the standard deviation and variance, the mean of the absolute value, and the l1 norm.","title":"Arguments"},{"location":"api/#configuration-using-sagemaker-python-sdk_1","text":"The reductions are passed as part of the \"reductions\" parameter to HookParameters or Collection Parameters. Refer Configuring Hook using SageMaker Python SDK and Configuring Collection using SageMaker Python SDK for more on that. The parameter \"reductions\" can take a comma separated string consisting of the following values: min max median mean std variance sum prod l1 l2 abs_min abs_max abs_median abs_mean abs_std abs_variance abs_sum abs_prod abs_l1 abs_l2","title":"Configuration using SageMaker Python SDK"},{"location":"api/#frameworks","text":"For details on what's supported for different framework, go here: * TensorFlow * PyTorch * MXNet * XGBoost","title":"Frameworks"},{"location":"distributed_training/","text":"Distributed Training TODO: Describe distributed training.","title":"Distributed Training"},{"location":"distributed_training/#distributed-training","text":"TODO: Describe distributed training.","title":"Distributed Training"},{"location":"env_var/","text":"Environment Variables USE_SMDEBUG : When using official SageMaker Framework Containers and AWS Deep Learning Containers which support the Zero Script Change experience , SageMaker Debugger can be disabled by setting this variable to 0 . In such a case, the hook is disabled regardless of what configuration is given to the job through the SageMaker Python SDK. By default this is set to 1 signifying True. SMDEBUG_CONFIG_FILE_PATH : Contains the path to the JSON file that describes the smdebug hook. At the minimum, the JSON config should contain the path where smdebug should output tensors. Example: { \"LocalPath\": \"/my/smdebug_hook/path\" } In SageMaker environment, this path is set to point to a pre-defined location containing a valid JSON. In non-SageMaker environment, SageMaker-Debugger is not used if this environment variable is not set and a hook is not created manually. Sample JSON from which a hook can be created: { \"LocalPath\": \"/my/smdebug_hook/path\", \"HookParameters\": { \"save_all\": false, \"include_regex\": \"regex1,regex2\", \"save_interval\": \"100\", \"save_steps\": \"1,2,3,4\", \"start_step\": \"1\", \"end_step\": \"1000000\", \"reductions\": \"min,max,mean\" }, \"CollectionConfigurations\": [ { \"CollectionName\": \"collection_obj_name1\", \"CollectionParameters\": { \"include_regex\": \"regexe5*\", \"save_interval\": 100, \"save_steps\": \"1,2,3\", \"start_step\": 1, \"reductions\": \"min\" } }, ] } TENSORBOARD_CONFIG_FILE_PATH : Contains the path to the JSON file that specifies where TensorBoard artifacts need to be placed. Sample JSON file: { \"LocalPath\": \"/my/tensorboard/path\" } In SageMaker environment, the presence of this JSON is necessary to log any Tensorboard artifact. By default, this path is set to point to a pre-defined location in SageMaker. tensorboard_dir can also be passed while creating the hook using the API or in the JSON specified in SMDEBUG_CONFIG_FILE_PATH. For this, export_tensorboard should be set to True. This option to set tensorboard_dir is available in both, SageMaker and non-SageMaker environments. CHECKPOINT_CONFIG_FILE_PATH : Contains the path to the JSON file that specifies where training checkpoints need to be placed. This is used in the context of spot training. Sample JSON file: { \"LocalPath\": \"/my/checkpoint/path\" } In SageMaker environment, the presence of this JSON is necessary to save checkpoints. By default, this path is set to point to a pre-defined location in SageMaker. SAGEMAKER_METRICS_DIRECTORY : Contains the path to the directory where metrics will be recorded for consumption by SageMaker Metrics. This is relevant only in SageMaker environment, where this variable points to a pre-defined location. Note : The environment variables below are applicable for versions > 0.4.14 SMDEBUG_TRAINING_END_DELAY_REFRESH : During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. This environment variable specifies how many seconds to wait before refreshing the index files to check if training has ended and the tensor is available. By default value, this value is set to 1. SMDEBUG_INCOMPLETE_STEP_WAIT_WINDOW : During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. A trial checks to see if a step specified in the smdebug hook has been completed. This environment variable specifies the maximum number of incomplete steps that the trial will wait for before marking half of them as complete. Default: 1000 SMDEBUG_MISSING_EVENT_FILE_RETRY_LIMIT : During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. All the tensor data is stored in the event files. When tensor data contained in an event file that is not available has been requested, this variable specifcies the number of times we retry the request.","title":"Env var"},{"location":"env_var/#environment-variables","text":"","title":"Environment Variables"},{"location":"env_var/#use_smdebug","text":"When using official SageMaker Framework Containers and AWS Deep Learning Containers which support the Zero Script Change experience , SageMaker Debugger can be disabled by setting this variable to 0 . In such a case, the hook is disabled regardless of what configuration is given to the job through the SageMaker Python SDK. By default this is set to 1 signifying True.","title":"USE_SMDEBUG:"},{"location":"env_var/#smdebug_config_file_path","text":"Contains the path to the JSON file that describes the smdebug hook. At the minimum, the JSON config should contain the path where smdebug should output tensors. Example: { \"LocalPath\": \"/my/smdebug_hook/path\" } In SageMaker environment, this path is set to point to a pre-defined location containing a valid JSON. In non-SageMaker environment, SageMaker-Debugger is not used if this environment variable is not set and a hook is not created manually. Sample JSON from which a hook can be created: { \"LocalPath\": \"/my/smdebug_hook/path\", \"HookParameters\": { \"save_all\": false, \"include_regex\": \"regex1,regex2\", \"save_interval\": \"100\", \"save_steps\": \"1,2,3,4\", \"start_step\": \"1\", \"end_step\": \"1000000\", \"reductions\": \"min,max,mean\" }, \"CollectionConfigurations\": [ { \"CollectionName\": \"collection_obj_name1\", \"CollectionParameters\": { \"include_regex\": \"regexe5*\", \"save_interval\": 100, \"save_steps\": \"1,2,3\", \"start_step\": 1, \"reductions\": \"min\" } }, ] }","title":"SMDEBUG_CONFIG_FILE_PATH:"},{"location":"env_var/#tensorboard_config_file_path","text":"Contains the path to the JSON file that specifies where TensorBoard artifacts need to be placed. Sample JSON file: { \"LocalPath\": \"/my/tensorboard/path\" } In SageMaker environment, the presence of this JSON is necessary to log any Tensorboard artifact. By default, this path is set to point to a pre-defined location in SageMaker. tensorboard_dir can also be passed while creating the hook using the API or in the JSON specified in SMDEBUG_CONFIG_FILE_PATH. For this, export_tensorboard should be set to True. This option to set tensorboard_dir is available in both, SageMaker and non-SageMaker environments.","title":"TENSORBOARD_CONFIG_FILE_PATH:"},{"location":"env_var/#checkpoint_config_file_path","text":"Contains the path to the JSON file that specifies where training checkpoints need to be placed. This is used in the context of spot training. Sample JSON file: { \"LocalPath\": \"/my/checkpoint/path\" } In SageMaker environment, the presence of this JSON is necessary to save checkpoints. By default, this path is set to point to a pre-defined location in SageMaker.","title":"CHECKPOINT_CONFIG_FILE_PATH:"},{"location":"env_var/#sagemaker_metrics_directory","text":"Contains the path to the directory where metrics will be recorded for consumption by SageMaker Metrics. This is relevant only in SageMaker environment, where this variable points to a pre-defined location. Note : The environment variables below are applicable for versions > 0.4.14","title":"SAGEMAKER_METRICS_DIRECTORY:"},{"location":"env_var/#smdebug_training_end_delay_refresh","text":"During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. This environment variable specifies how many seconds to wait before refreshing the index files to check if training has ended and the tensor is available. By default value, this value is set to 1.","title":"SMDEBUG_TRAINING_END_DELAY_REFRESH:"},{"location":"env_var/#smdebug_incomplete_step_wait_window","text":"During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. A trial checks to see if a step specified in the smdebug hook has been completed. This environment variable specifies the maximum number of incomplete steps that the trial will wait for before marking half of them as complete. Default: 1000","title":"SMDEBUG_INCOMPLETE_STEP_WAIT_WINDOW:"},{"location":"env_var/#smdebug_missing_event_file_retry_limit","text":"During analysis, a trial is created to query for tensors from a specified directory. This directory contains collections, events, and index files. All the tensor data is stored in the event files. When tensor data contained in an event file that is not available has been requested, this variable specifcies the number of times we retry the request.","title":"SMDEBUG_MISSING_EVENT_FILE_RETRY_LIMIT:"},{"location":"mxnet/","text":"MXNet Contents Support How to Use Example Full API Support Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for MXNet 1.6 , or the AWS Deep Learning Container for MXNet 1.6 . This library itself supports the following versions when you use our API which requires a few minimal changes to your training script: MXNet 1.4, 1.5, 1.6. Only Gluon models are supported When the Gluon model is hybridized, inputs and outputs of intermediate layers can not be saved Parameter server based distributed training is not yet supported How to Use Using Zero Script Change containers In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.mxnet as smd hook = smd.Hook.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers. Bring your own container experience 1. Create a hook If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.Hook.create_from_json_file() . Otherwise, call the hook class constructor, smd.Hook() . 2. Register the model to the hook Call hook.register_block(net) . 3. (Optional) Configure Collections, SaveConfig and ReductionConfig See the Common API page for details on how to do this. Example ####################################### # Creating a hook. Refer `API for Saving Tensors` page for more on this import smdebug.mxnet as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### import mxnet as mx from mxnet import gluon from mxnet import autograd as ag from mxnet.gluon import nn net = nn.HybridSequential() net.add( nn.Dense(128, activation='relu'), nn.Dense(64, activation='relu'), nn.Dense(10, activation=\"relu\"), ) net.initialize(init=init.Xavier(), ctx=mx.cpu()) softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': args.lr}) ####################################### # Here we register the block to smdebug hook.register_block(net) ####################################### batch_size = 100 mnist = mx.test_utils.get_mnist() train_data = mx.io.NDArrayIter(mnist['train_data'], mnist['train_label'], batch_size, shuffle=True) val_data = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size) for i in range(args.epochs): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator. for batch in train_data: data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0) label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0) outputs = [] with ag.record(): for x, y in zip(data, label): z = net(x) loss = softmax_cross_entropy_loss(z, y) loss.backward() outputs.append(z) metric.update(label, outputs) trainer.step(batch.data[0].shape[0]) name, acc = metric.get() metric.reset() Full API See the API for Saving Tensors page for details about Hook, Collection, SaveConfig, and ReductionConfig See the Analysis page for details about analyzing a training job.","title":"MXNet"},{"location":"mxnet/#mxnet","text":"","title":"MXNet"},{"location":"mxnet/#contents","text":"Support How to Use Example Full API","title":"Contents"},{"location":"mxnet/#support","text":"Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for MXNet 1.6 , or the AWS Deep Learning Container for MXNet 1.6 . This library itself supports the following versions when you use our API which requires a few minimal changes to your training script: MXNet 1.4, 1.5, 1.6. Only Gluon models are supported When the Gluon model is hybridized, inputs and outputs of intermediate layers can not be saved Parameter server based distributed training is not yet supported","title":"Support"},{"location":"mxnet/#how-to-use","text":"","title":"How to Use"},{"location":"mxnet/#using-zero-script-change-containers","text":"In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.mxnet as smd hook = smd.Hook.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers.","title":"Using Zero Script Change containers"},{"location":"mxnet/#bring-your-own-container-experience","text":"","title":"Bring your own container experience"},{"location":"mxnet/#1-create-a-hook","text":"If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.Hook.create_from_json_file() . Otherwise, call the hook class constructor, smd.Hook() .","title":"1. Create a hook"},{"location":"mxnet/#2-register-the-model-to-the-hook","text":"Call hook.register_block(net) .","title":"2. Register the model to the hook"},{"location":"mxnet/#3-optional-configure-collections-saveconfig-and-reductionconfig","text":"See the Common API page for details on how to do this.","title":"3. (Optional) Configure Collections, SaveConfig and ReductionConfig"},{"location":"mxnet/#example","text":"####################################### # Creating a hook. Refer `API for Saving Tensors` page for more on this import smdebug.mxnet as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### import mxnet as mx from mxnet import gluon from mxnet import autograd as ag from mxnet.gluon import nn net = nn.HybridSequential() net.add( nn.Dense(128, activation='relu'), nn.Dense(64, activation='relu'), nn.Dense(10, activation=\"relu\"), ) net.initialize(init=init.Xavier(), ctx=mx.cpu()) softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': args.lr}) ####################################### # Here we register the block to smdebug hook.register_block(net) ####################################### batch_size = 100 mnist = mx.test_utils.get_mnist() train_data = mx.io.NDArrayIter(mnist['train_data'], mnist['train_label'], batch_size, shuffle=True) val_data = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size) for i in range(args.epochs): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator. for batch in train_data: data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0) label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0) outputs = [] with ag.record(): for x, y in zip(data, label): z = net(x) loss = softmax_cross_entropy_loss(z, y) loss.backward() outputs.append(z) metric.update(label, outputs) trainer.step(batch.data[0].shape[0]) name, acc = metric.get() metric.reset()","title":"Example"},{"location":"mxnet/#full-api","text":"See the API for Saving Tensors page for details about Hook, Collection, SaveConfig, and ReductionConfig See the Analysis page for details about analyzing a training job.","title":"Full API"},{"location":"pytorch/","text":"PyTorch Contents Support How to Use Module Loss Example Functional Loss Example Full API Support Versions Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for PyTorch 1.3 , or the AWS Deep Learning Container for PyTorch 1.3 . The library itself supports the following versions when using changes to the training script: PyTorch 1.2, 1.3. How to Use Using Zero Script Change containers In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.pytorch as smd hook = smd.Hook.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers. Bring your own container experience 1. Create a hook If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.Hook.create_from_json_file() . Otherwise, call the hook class constructor, smd.Hook() . 2. Register the model to the hook Call hook.register_module(net) . 3. Register your loss function to the hook If using a loss which is a subclass of nn.Module , call hook.register_loss(loss_criterion) once before starting training.\\ If using a loss which is a subclass of nn.functional , call hook.record_tensor_value(loss) after each training step. 4. (Optional) Configure Collections, SaveConfig and ReductionConfig See the Common API page for details on how to do this. Module Loss Example ####################################### # Creating a hook. Refer `API for Saving Tensors` page for more on this import smdebug.pytorch as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### class Model(nn.Module) def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return F.relu(self.fc(x)) net = Model() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=args.lr) ####################################### # Register the hook and the loss hook.register_module(net) hook.register_loss(criterion) ####################################### # Training loop as usual for (inputs, labels) in trainloader: optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() Functional Loss Example ####################################### # Register the hook and the loss import smdebug.pytorch as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### class Model(nn.Module) def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return F.relu(self.fc(x)) net = Model() optimizer = optim.Adam(net.parameters(), lr=args.lr) ####################################### # Register the hook hook.register_module(net) ####################################### # Training loop, recording the loss at each iteration for (inputs, labels) in trainloader: optimizer.zero_grad() outputs = net(inputs) loss = F.cross_entropy(outputs, labels) ####################################### # Manually record the loss hook.record_tensor_value(tensor_name=\"loss\", tensor_value=loss) ####################################### loss.backward() optimizer.step() Full API See the API for Saving Tensors page for details about Hook, Collection, SaveConfig, and ReductionConfig. See the Analysis page for details about analyzing a training job.","title":"PyTorch"},{"location":"pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"pytorch/#contents","text":"Support How to Use Module Loss Example Functional Loss Example Full API","title":"Contents"},{"location":"pytorch/#support","text":"","title":"Support"},{"location":"pytorch/#versions","text":"Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for PyTorch 1.3 , or the AWS Deep Learning Container for PyTorch 1.3 . The library itself supports the following versions when using changes to the training script: PyTorch 1.2, 1.3.","title":"Versions"},{"location":"pytorch/#how-to-use","text":"","title":"How to Use"},{"location":"pytorch/#using-zero-script-change-containers","text":"In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.pytorch as smd hook = smd.Hook.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers.","title":"Using Zero Script Change containers"},{"location":"pytorch/#bring-your-own-container-experience","text":"","title":"Bring your own container experience"},{"location":"pytorch/#1-create-a-hook","text":"If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.Hook.create_from_json_file() . Otherwise, call the hook class constructor, smd.Hook() .","title":"1. Create a hook"},{"location":"pytorch/#2-register-the-model-to-the-hook","text":"Call hook.register_module(net) .","title":"2. Register the model to the hook"},{"location":"pytorch/#3-register-your-loss-function-to-the-hook","text":"If using a loss which is a subclass of nn.Module , call hook.register_loss(loss_criterion) once before starting training.\\ If using a loss which is a subclass of nn.functional , call hook.record_tensor_value(loss) after each training step.","title":"3. Register your loss function to the hook"},{"location":"pytorch/#4-optional-configure-collections-saveconfig-and-reductionconfig","text":"See the Common API page for details on how to do this.","title":"4. (Optional) Configure Collections, SaveConfig and ReductionConfig"},{"location":"pytorch/#module-loss-example","text":"####################################### # Creating a hook. Refer `API for Saving Tensors` page for more on this import smdebug.pytorch as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### class Model(nn.Module) def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return F.relu(self.fc(x)) net = Model() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=args.lr) ####################################### # Register the hook and the loss hook.register_module(net) hook.register_loss(criterion) ####################################### # Training loop as usual for (inputs, labels) in trainloader: optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step()","title":"Module Loss Example"},{"location":"pytorch/#functional-loss-example","text":"####################################### # Register the hook and the loss import smdebug.pytorch as smd hook = smd.Hook(out_dir=args.out_dir) ####################################### class Model(nn.Module) def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return F.relu(self.fc(x)) net = Model() optimizer = optim.Adam(net.parameters(), lr=args.lr) ####################################### # Register the hook hook.register_module(net) ####################################### # Training loop, recording the loss at each iteration for (inputs, labels) in trainloader: optimizer.zero_grad() outputs = net(inputs) loss = F.cross_entropy(outputs, labels) ####################################### # Manually record the loss hook.record_tensor_value(tensor_name=\"loss\", tensor_value=loss) ####################################### loss.backward() optimizer.step()","title":"Functional Loss Example"},{"location":"pytorch/#full-api","text":"See the API for Saving Tensors page for details about Hook, Collection, SaveConfig, and ReductionConfig. See the Analysis page for details about analyzing a training job.","title":"Full API"},{"location":"sagemaker/","text":"Running SageMaker jobs with Amazon SageMaker Debugger Outline Configuring SageMaker Debugger Saving data Saving built-in collections that we manage Saving reductions for a custom collection Enabling TensorBoard summaries Rules Built In Rules Custom Rules Interactive Exploration SageMaker Studio TensorBoard Visualization Example Notebooks Configuring SageMaker Debugger Regardless of which of the two above ways you have enabled SageMaker Debugger, you can configure it using the SageMaker python SDK. There are two aspects to this configuration. - You can specify what tensors to be saved, when they should be saved and in what form they should be saved. - You can specify which Rule you want to monitor your training job with. This can be either a built in rule that SageMaker provides, or a custom rule that you can write yourself. Saving Data SageMaker Debugger gives you a powerful and flexible API to save the tensors you choose at the frequencies you want. These configurations are made available in the SageMaker Python SDK through the DebuggerHookConfig class. Saving built-in collections that we manage Learn more about these built in collections here . from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', hook_parameters={ \"save_interval\": 100 }, collection_configs=[ CollectionConfig(\"weights\"), CollectionConfig(\"gradients\"), CollectionConfig(\"losses\"), CollectionConfig( name=\"biases\", parameters={ \"save_interval\": 10, \"end_step\": 500 } ), ] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config ) sagemaker_estimator.fit() Saving reductions for a custom collection You can define your collection of tensors. You can also choose to save certain reductions of tensors only instead of saving the full tensor. You may choose to do this to reduce the amount of data saved. Please note that when you save reductions, unless you pass the flag save_raw_tensor , only these reductions will be available for analysis. The raw tensor will not be saved. from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig( name=\"activations\", parameters={ \"include_regex\": \"relu|tanh\", \"reductions\": \"mean,variance,max,abs_mean,abs_variance,abs_max\" }) ] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config ) sagemaker_estimator.fit() Enabling TensorBoard summaries SageMaker Debugger can automatically generate tensorboard scalar summaries, distributions and histograms for tensors saved. This can be enabled by passing a TensorBoardOutputConfig object when creating an Estimator as follows. You can also choose to disable or enable histograms specifically for different collections. By default a collection has save_histogram flag set to True. Note that scalar summaries are added to TensorBoard for all ScalarCollections and any scalar saved through hook.save_scalar . Refer API for more details on scalar collections and save_scalar method. The below example saves weights and gradients as full tensors, and also saves the gradients as histograms and distributions to visualize in TensorBoard. These will be saved to the location passed in TensorBoardOutputConfig object. from sagemaker.debugger import DebuggerHookConfig, CollectionConfig, TensorBoardOutputConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig( name=\"weights\", parameters={\"save_histogram\": False}), CollectionConfig(name=\"gradients\"), ] ) tb_config = TensorBoardOutputConfig('s3://smdebug-dev-demo-pdx/mnist/tensorboard') import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config, tensorboard_output_config=tb_config ) sagemaker_estimator.fit() For more details, refer our API page . Rules Here are some examples on how to run Rules with your training jobs. Note that passing a CollectionConfig object to the Rule as collections_to_save is equivalent to passing it to the DebuggerHookConfig object as collection_configs . This is just a shortcut for your convenience. Built in Rules The Built-in Rules, or SageMaker Rules, are described in detail on this page Scope of Validity Rules Generic Deep Learning models (TensorFlow, Apache MXNet, and PyTorch) dead_relu exploding_tensor poor_weight_initialization saturated_activation vanishing_gradient weight_update_ratio Generic Deep learning models (TensorFlow, MXNet, and PyTorch) and the XGBoost algorithm all_zero class_imbalance confusion loss_not_decreasing overfit overtraining similar_across_runs tensor_variance unchanged_tensor Deep learning applications check_input_images nlp_sequence_ratio XGBoost algorithm tree_depth Running built-in SageMaker Rules You can run a SageMaker built-in Rule as follows using the Rule.sagemaker method. The first argument to this method is the base configuration that is associated with the Rule. We configure them as much as possible. You can take a look at the ruleconfigs that we populate for all built-in rules here . You can choose to customize these parameters using the other parameters. These rules are run on our pre-built Docker images which are listed here . You are not charged for the instances when running SageMaker built-in rules. A list of all our built-in rules are provided below . from sagemaker.debugger import Rule, CollectionConfig, rule_configs exploding_tensor_rule = Rule.sagemaker( base_config=rule_configs.exploding_tensor(), rule_parameters={\"collection_names\": \"weights,losses\"}, collections_to_save=[ CollectionConfig(\"weights\"), CollectionConfig(\"losses\") ] ) vanishing_gradient_rule = Rule.sagemaker( base_config=rule_configs.vanishing_gradient() ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below rules=[exploding_tensor_rule, vanishing_gradient_rule], ) sagemaker_estimator.fit() Custom Rules You can write your own rule custom made for your application and provide it, so SageMaker can monitor your training job using your rule. To do so, you need to understand the programming model that smdebug provides. Our page on Programming Model for Analysis describes the APIs that we provide to help you write your own rule. Please refer to this example notebook for a demonstration of creating your custom rule and running it on SageMaker. Running custom Rules To run a custom rule, you have to provide a few additional parameters. Key parameters to note are a file which has the implementation of your Rule class source , the name of the Rule class ( rule_to_invoke ), the type of instance to run the Rule job on ( instance_type ), the size of the volume on that instance ( volume_size_in_gb ), and the docker image to use for running this job ( image_uri ). Please refer to the documentation here for more details. We have pre-built Docker images that you can use to run your custom rules. These are listed here . You can also choose to build your own Docker image for custom rule evaluation. Please refer to the repository SageMaker Debugger Rules Container for instructions on how to build such an image. from sagemaker.debugger import Rule, CollectionConfig custom_coll = CollectionConfig( name=\"relu_activations\", parameters={ \"include_regex\": \"relu\", \"save_interval\": 500, \"end_step\": 5000 }) improper_activation_rule = Rule.custom( name='improper_activation_job', image_uri='552407032007.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', instance_type='ml.c4.xlarge', volume_size_in_gb=400, source='rules/custom_rules.py', rule_to_invoke='ImproperActivation', rule_parameters={\"collection_names\": \"relu_activations\"}, collections_to_save=[custom_coll] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below rules=[improper_activation_rule], ) sagemaker_estimator.fit() For more details, refer our Analysis page . Interactive Exploration smdebug SDK also allows you perform interactive and real-time exploration of the data saved. You can choose to inspect the tensors saved, or visualize them through your custom plots. You can retrieve these tensors as numpy arrays allowing you to use your favorite analysis libraries right in a SageMaker notebook instance. We have couple of example notebooks demonstrating this. - Real-time anaysis in a notebook during training - Interactive tensor analysis in a notebook SageMaker Studio SageMaker Debugger is on by default for supported training jobs on the official SageMaker Framework containers (or AWS Deep Learning Containers) during SageMaker training jobs. In this default scenario, SageMaker Debugger takes the losses and metrics from your training job and publishes them to SageMaker Metrics, allowing you to track these metrics in SageMaker Studio. You can also see the status of Rules you have enabled for your training job right in the Studio. Here are screenshots of that experience. TensorBoard Visualization If you have enabled TensorBoard outputs for your training job through SageMaker Debugger, TensorBoard artifacts will automatically be generated for the tensors saved. You can then point your TensorBoard instance to that S3 location and review the visualizations for the tensors saved. Example Notebooks We have a bunch of example notebooks here demonstrating different aspects of SageMaker Debugger.","title":"Sagemaker"},{"location":"sagemaker/#running-sagemaker-jobs-with-amazon-sagemaker-debugger","text":"","title":"Running SageMaker jobs with Amazon SageMaker Debugger"},{"location":"sagemaker/#outline","text":"Configuring SageMaker Debugger Saving data Saving built-in collections that we manage Saving reductions for a custom collection Enabling TensorBoard summaries Rules Built In Rules Custom Rules Interactive Exploration SageMaker Studio TensorBoard Visualization Example Notebooks","title":"Outline"},{"location":"sagemaker/#configuring-sagemaker-debugger","text":"Regardless of which of the two above ways you have enabled SageMaker Debugger, you can configure it using the SageMaker python SDK. There are two aspects to this configuration. - You can specify what tensors to be saved, when they should be saved and in what form they should be saved. - You can specify which Rule you want to monitor your training job with. This can be either a built in rule that SageMaker provides, or a custom rule that you can write yourself.","title":"Configuring SageMaker Debugger"},{"location":"sagemaker/#saving-data","text":"SageMaker Debugger gives you a powerful and flexible API to save the tensors you choose at the frequencies you want. These configurations are made available in the SageMaker Python SDK through the DebuggerHookConfig class.","title":"Saving Data"},{"location":"sagemaker/#saving-built-in-collections-that-we-manage","text":"Learn more about these built in collections here . from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', hook_parameters={ \"save_interval\": 100 }, collection_configs=[ CollectionConfig(\"weights\"), CollectionConfig(\"gradients\"), CollectionConfig(\"losses\"), CollectionConfig( name=\"biases\", parameters={ \"save_interval\": 10, \"end_step\": 500 } ), ] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config ) sagemaker_estimator.fit()","title":"Saving built-in collections that we manage"},{"location":"sagemaker/#saving-reductions-for-a-custom-collection","text":"You can define your collection of tensors. You can also choose to save certain reductions of tensors only instead of saving the full tensor. You may choose to do this to reduce the amount of data saved. Please note that when you save reductions, unless you pass the flag save_raw_tensor , only these reductions will be available for analysis. The raw tensor will not be saved. from sagemaker.debugger import DebuggerHookConfig, CollectionConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig( name=\"activations\", parameters={ \"include_regex\": \"relu|tanh\", \"reductions\": \"mean,variance,max,abs_mean,abs_variance,abs_max\" }) ] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config ) sagemaker_estimator.fit()","title":"Saving reductions for a custom collection"},{"location":"sagemaker/#enabling-tensorboard-summaries","text":"SageMaker Debugger can automatically generate tensorboard scalar summaries, distributions and histograms for tensors saved. This can be enabled by passing a TensorBoardOutputConfig object when creating an Estimator as follows. You can also choose to disable or enable histograms specifically for different collections. By default a collection has save_histogram flag set to True. Note that scalar summaries are added to TensorBoard for all ScalarCollections and any scalar saved through hook.save_scalar . Refer API for more details on scalar collections and save_scalar method. The below example saves weights and gradients as full tensors, and also saves the gradients as histograms and distributions to visualize in TensorBoard. These will be saved to the location passed in TensorBoardOutputConfig object. from sagemaker.debugger import DebuggerHookConfig, CollectionConfig, TensorBoardOutputConfig hook_config = DebuggerHookConfig( s3_output_path='s3://smdebug-dev-demo-pdx/mnist', collection_configs=[ CollectionConfig( name=\"weights\", parameters={\"save_histogram\": False}), CollectionConfig(name=\"gradients\"), ] ) tb_config = TensorBoardOutputConfig('s3://smdebug-dev-demo-pdx/mnist/tensorboard') import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below debugger_hook_config=hook_config, tensorboard_output_config=tb_config ) sagemaker_estimator.fit() For more details, refer our API page .","title":"Enabling TensorBoard summaries"},{"location":"sagemaker/#rules","text":"Here are some examples on how to run Rules with your training jobs. Note that passing a CollectionConfig object to the Rule as collections_to_save is equivalent to passing it to the DebuggerHookConfig object as collection_configs . This is just a shortcut for your convenience.","title":"Rules"},{"location":"sagemaker/#built-in-rules","text":"The Built-in Rules, or SageMaker Rules, are described in detail on this page Scope of Validity Rules Generic Deep Learning models (TensorFlow, Apache MXNet, and PyTorch) dead_relu exploding_tensor poor_weight_initialization saturated_activation vanishing_gradient weight_update_ratio Generic Deep learning models (TensorFlow, MXNet, and PyTorch) and the XGBoost algorithm all_zero class_imbalance confusion loss_not_decreasing overfit overtraining similar_across_runs tensor_variance unchanged_tensor Deep learning applications check_input_images nlp_sequence_ratio XGBoost algorithm tree_depth","title":"Built in Rules"},{"location":"sagemaker/#running-built-in-sagemaker-rules","text":"You can run a SageMaker built-in Rule as follows using the Rule.sagemaker method. The first argument to this method is the base configuration that is associated with the Rule. We configure them as much as possible. You can take a look at the ruleconfigs that we populate for all built-in rules here . You can choose to customize these parameters using the other parameters. These rules are run on our pre-built Docker images which are listed here . You are not charged for the instances when running SageMaker built-in rules. A list of all our built-in rules are provided below . from sagemaker.debugger import Rule, CollectionConfig, rule_configs exploding_tensor_rule = Rule.sagemaker( base_config=rule_configs.exploding_tensor(), rule_parameters={\"collection_names\": \"weights,losses\"}, collections_to_save=[ CollectionConfig(\"weights\"), CollectionConfig(\"losses\") ] ) vanishing_gradient_rule = Rule.sagemaker( base_config=rule_configs.vanishing_gradient() ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below rules=[exploding_tensor_rule, vanishing_gradient_rule], ) sagemaker_estimator.fit()","title":"Running built-in SageMaker Rules"},{"location":"sagemaker/#custom-rules","text":"You can write your own rule custom made for your application and provide it, so SageMaker can monitor your training job using your rule. To do so, you need to understand the programming model that smdebug provides. Our page on Programming Model for Analysis describes the APIs that we provide to help you write your own rule. Please refer to this example notebook for a demonstration of creating your custom rule and running it on SageMaker.","title":"Custom Rules"},{"location":"sagemaker/#running-custom-rules","text":"To run a custom rule, you have to provide a few additional parameters. Key parameters to note are a file which has the implementation of your Rule class source , the name of the Rule class ( rule_to_invoke ), the type of instance to run the Rule job on ( instance_type ), the size of the volume on that instance ( volume_size_in_gb ), and the docker image to use for running this job ( image_uri ). Please refer to the documentation here for more details. We have pre-built Docker images that you can use to run your custom rules. These are listed here . You can also choose to build your own Docker image for custom rule evaluation. Please refer to the repository SageMaker Debugger Rules Container for instructions on how to build such an image. from sagemaker.debugger import Rule, CollectionConfig custom_coll = CollectionConfig( name=\"relu_activations\", parameters={ \"include_regex\": \"relu\", \"save_interval\": 500, \"end_step\": 5000 }) improper_activation_rule = Rule.custom( name='improper_activation_job', image_uri='552407032007.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', instance_type='ml.c4.xlarge', volume_size_in_gb=400, source='rules/custom_rules.py', rule_to_invoke='ImproperActivation', rule_parameters={\"collection_names\": \"relu_activations\"}, collections_to_save=[custom_coll] ) import sagemaker as sm sagemaker_estimator = sm.tensorflow.TensorFlow( entry_point='src/mnist.py', role=sm.get_execution_role(), base_job_name='smdebug-demo-job', train_instance_count=1, train_instance_type=\"ml.m4.xlarge\", framework_version=\"1.15\", py_version=\"py3\", # smdebug-specific arguments below rules=[improper_activation_rule], ) sagemaker_estimator.fit() For more details, refer our Analysis page .","title":"Running custom Rules"},{"location":"sagemaker/#interactive-exploration","text":"smdebug SDK also allows you perform interactive and real-time exploration of the data saved. You can choose to inspect the tensors saved, or visualize them through your custom plots. You can retrieve these tensors as numpy arrays allowing you to use your favorite analysis libraries right in a SageMaker notebook instance. We have couple of example notebooks demonstrating this. - Real-time anaysis in a notebook during training - Interactive tensor analysis in a notebook","title":"Interactive Exploration"},{"location":"sagemaker/#sagemaker-studio","text":"SageMaker Debugger is on by default for supported training jobs on the official SageMaker Framework containers (or AWS Deep Learning Containers) during SageMaker training jobs. In this default scenario, SageMaker Debugger takes the losses and metrics from your training job and publishes them to SageMaker Metrics, allowing you to track these metrics in SageMaker Studio. You can also see the status of Rules you have enabled for your training job right in the Studio. Here are screenshots of that experience.","title":"SageMaker Studio"},{"location":"sagemaker/#tensorboard-visualization","text":"If you have enabled TensorBoard outputs for your training job through SageMaker Debugger, TensorBoard artifacts will automatically be generated for the tensors saved. You can then point your TensorBoard instance to that S3 location and review the visualizations for the tensors saved.","title":"TensorBoard Visualization"},{"location":"sagemaker/#example-notebooks","text":"We have a bunch of example notebooks here demonstrating different aspects of SageMaker Debugger.","title":"Example Notebooks"},{"location":"tensorflow/","text":"Tensorflow Contents Support How to Use tf.keras Example MonitoredSession Example Estimator Example Full API Support Versions Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for TensorFlow 1.15 , or the AWS Deep Learning Container for TensorFlow 1.15 . This library itself supports the following versions when you use our API which requires a few minimal changes to your training script: TensorFlow 1.14, 1.15, 2.0.1, 2.1.0. Keras 2.3. Interfaces Estimator tf.keras MonitoredSession Distributed training MirroredStrategy or Contrib MirroredStrategy We will very quickly follow up with support for Horovod and Parameter Server based training. How to Use Using Zero Script Change containers In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.tensorflow as smd hook = smd.{hook_class}.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers. Bring your own container experience 1. Create a hook If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.{hook_class}.create_from_json_file() . Otherwise, call the hook class constructor, smd.{hook_class}() . Details are below for tf.keras, MonitoredSession, or Estimator. 2. Register the hook to your model The argument is callbacks=[hook] for tf.keras. It is hooks=[hook] for MonitoredSession and Estimator. 3. Wrap the optimizer If you would like to save gradients , wrap your optimizer with the hook as follows optimizer = hook.wrap_optimizer(optimizer) . This does not modify your optimization logic, and returns the same optimizer instance passed to the method. 4. (Optional) Configure Collections, SaveConfig and ReductionConfig See the Common API page for details on how to do this. Examples We have three Hooks for different interfaces of TensorFlow. The following is needed to enable SageMaker Debugger on non Zero Script Change supported containers. Refer SageMaker training on how to use the Zero Script Change experience. tf.keras Example import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', ) # Add the hook as a callback model.fit(x_train, y_train, epochs=args.epochs, callbacks=[hook]) model.evaluate(x_test, y_test, callbacks=[hook]) TF 2.x GradientTape example import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) for epoch in range(n_epochs): for data, labels in dataset: dataset_labels = labels # wrap the tape to capture tensors with hook.wrap_tape(tf.GradientTape(persistent=True)) as tape: logits = model(data, training=True) # (32,10) loss_value = cce(labels, logits) grads = tape.gradient(loss_value, model.variables) opt.apply_gradients(zip(grads, model.variables)) acc = train_acc_metric(dataset_labels, logits) # manually save metric values hook.record_tensor_value(tensor_name=\"accuracy\", tensor_value=acc) MonitoredSession Example import smdebug.tensorflow as smd hook = smd.SessionHook(out_dir=args.out_dir) loss = tf.reduce_mean(tf.matmul(...), name=\"loss\") optimizer = tf.train.AdamOptimizer(args.lr) # Wrap the optimizer optimizer = hook.wrap_optimizer(optimizer) # Add the hook as a callback sess = tf.train.MonitoredSession(hooks=[hook]) sess.run([loss, ...]) Estimator Example import smdebug.tensorflow as smd hook = smd.EstimatorHook(out_dir=args.out_dir) train_input_fn, eval_input_fn = ... estimator = tf.estimator.Estimator(...) # Set the mode and pass the hook as callback hook.set_mode(mode=smd.modes.TRAIN) estimator.train(input_fn=train_input_fn, steps=args.steps, hooks=[hook]) hook.set_mode(mode=smd.modes.EVAL) estimator.evaluate(input_fn=eval_input_fn, steps=args.steps, hooks=[hook]) Full API See the API for saving tensors page for details about the Hooks, Collection, SaveConfig, and ReductionConfig. See the Analysis page for details about analyzing a training job.","title":"Tensorflow"},{"location":"tensorflow/#tensorflow","text":"","title":"Tensorflow"},{"location":"tensorflow/#contents","text":"Support How to Use tf.keras Example MonitoredSession Example Estimator Example Full API","title":"Contents"},{"location":"tensorflow/#support","text":"","title":"Support"},{"location":"tensorflow/#versions","text":"Zero Script Change experience where you need no modifications to your training script is supported in the official SageMaker Framework Container for TensorFlow 1.15 , or the AWS Deep Learning Container for TensorFlow 1.15 . This library itself supports the following versions when you use our API which requires a few minimal changes to your training script: TensorFlow 1.14, 1.15, 2.0.1, 2.1.0. Keras 2.3.","title":"Versions"},{"location":"tensorflow/#interfaces","text":"Estimator tf.keras MonitoredSession","title":"Interfaces"},{"location":"tensorflow/#distributed-training","text":"MirroredStrategy or Contrib MirroredStrategy We will very quickly follow up with support for Horovod and Parameter Server based training.","title":"Distributed training"},{"location":"tensorflow/#how-to-use","text":"","title":"How to Use"},{"location":"tensorflow/#using-zero-script-change-containers","text":"In this case, you don't need to do anything to get the hook running. You are encouraged to configure the hook from the SageMaker python SDK so you can run different jobs with different configurations without having to modify your script. If you want access to the hook to configure certain things which can not be configured through the SageMaker SDK, you can retrieve the hook as follows. import smdebug.tensorflow as smd hook = smd.{hook_class}.create_from_json_file() Note that you can create the hook from smdebug's python API as is being done in the next section even in such containers.","title":"Using Zero Script Change containers"},{"location":"tensorflow/#bring-your-own-container-experience","text":"","title":"Bring your own container experience"},{"location":"tensorflow/#1-create-a-hook","text":"If using SageMaker, you will configure the hook in SageMaker's python SDK using the Estimator class. Instantiate it with smd.{hook_class}.create_from_json_file() . Otherwise, call the hook class constructor, smd.{hook_class}() . Details are below for tf.keras, MonitoredSession, or Estimator.","title":"1. Create a hook"},{"location":"tensorflow/#2-register-the-hook-to-your-model","text":"The argument is callbacks=[hook] for tf.keras. It is hooks=[hook] for MonitoredSession and Estimator.","title":"2. Register the hook to your model"},{"location":"tensorflow/#3-wrap-the-optimizer","text":"If you would like to save gradients , wrap your optimizer with the hook as follows optimizer = hook.wrap_optimizer(optimizer) . This does not modify your optimization logic, and returns the same optimizer instance passed to the method.","title":"3. Wrap the optimizer"},{"location":"tensorflow/#4-optional-configure-collections-saveconfig-and-reductionconfig","text":"See the Common API page for details on how to do this.","title":"4. (Optional) Configure Collections, SaveConfig and ReductionConfig"},{"location":"tensorflow/#examples","text":"We have three Hooks for different interfaces of TensorFlow. The following is needed to enable SageMaker Debugger on non Zero Script Change supported containers. Refer SageMaker training on how to use the Zero Script Change experience.","title":"Examples"},{"location":"tensorflow/#tfkeras","text":"","title":"tf.keras"},{"location":"tensorflow/#example","text":"import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', ) # Add the hook as a callback model.fit(x_train, y_train, epochs=args.epochs, callbacks=[hook]) model.evaluate(x_test, y_test, callbacks=[hook])","title":"Example"},{"location":"tensorflow/#tf-2x-gradienttape-example","text":"import smdebug.tensorflow as smd hook = smd.KerasHook(out_dir=args.out_dir) model = tf.keras.models.Sequential([ ... ]) for epoch in range(n_epochs): for data, labels in dataset: dataset_labels = labels # wrap the tape to capture tensors with hook.wrap_tape(tf.GradientTape(persistent=True)) as tape: logits = model(data, training=True) # (32,10) loss_value = cce(labels, logits) grads = tape.gradient(loss_value, model.variables) opt.apply_gradients(zip(grads, model.variables)) acc = train_acc_metric(dataset_labels, logits) # manually save metric values hook.record_tensor_value(tensor_name=\"accuracy\", tensor_value=acc)","title":"TF 2.x GradientTape example"},{"location":"tensorflow/#monitoredsession","text":"","title":"MonitoredSession"},{"location":"tensorflow/#example_1","text":"import smdebug.tensorflow as smd hook = smd.SessionHook(out_dir=args.out_dir) loss = tf.reduce_mean(tf.matmul(...), name=\"loss\") optimizer = tf.train.AdamOptimizer(args.lr) # Wrap the optimizer optimizer = hook.wrap_optimizer(optimizer) # Add the hook as a callback sess = tf.train.MonitoredSession(hooks=[hook]) sess.run([loss, ...])","title":"Example"},{"location":"tensorflow/#estimator","text":"","title":"Estimator"},{"location":"tensorflow/#example_2","text":"import smdebug.tensorflow as smd hook = smd.EstimatorHook(out_dir=args.out_dir) train_input_fn, eval_input_fn = ... estimator = tf.estimator.Estimator(...) # Set the mode and pass the hook as callback hook.set_mode(mode=smd.modes.TRAIN) estimator.train(input_fn=train_input_fn, steps=args.steps, hooks=[hook]) hook.set_mode(mode=smd.modes.EVAL) estimator.evaluate(input_fn=eval_input_fn, steps=args.steps, hooks=[hook])","title":"Example"},{"location":"tensorflow/#full-api","text":"See the API for saving tensors page for details about the Hooks, Collection, SaveConfig, and ReductionConfig. See the Analysis page for details about analyzing a training job.","title":"Full API"},{"location":"xgboost/","text":"XGBoost Contents SageMaker Example Full API SageMaker Example Use XGBoost as a built-in algorithm The XGBoost algorithm can be used 1) as a built-in algorithm, or 2) as a framework such as MXNet, PyTorch, or Tensorflow. If SageMaker XGBoost is used as a built-in algorithm in container version 0.90-2 or later, Amazon SageMaker Debugger will be available by default (i.e., zero code change experience). See XGBoost Algorithm AWS docmentation for more information on how to use XGBoost as a built-in algorithm. See Amazon SageMaker Debugger examples for sample notebooks that demonstrate debugging and monitoring capabilities of Amazon SageMaker Debugger. See SageMaker Python SDK for more information on how to configure the Amazon SageMaker Debugger from the Python SDK. Use XGBoost as a framework When SageMaker XGBoost is used as a framework, it is recommended that the hook is configured from the SageMaker Python SDK . By using SageMaker Python SDK, you can run different jobs (e.g., Processing jobs) on the SageMaker platform. You can retrieve the hook as follows. import xgboost as xgb from smdebug.xgboost import Hook dtrain = xgb.DMatrix(\"train.libsvm\") dtest = xgb.DMatrix(\"test.libsmv\") hook = Hook.create_from_json_file() hook.train_data = dtrain # required hook.validation_data = dtest # optional hook.hyperparameters = params # optional bst = xgb.train( params, dtrain, callbacks=[hook], evals_result=[(dtrain, \"train\"), (dvalid, \"validation\")] ) Alternatively, you can also create the hook from smdebug 's Python API as shown in the next section. Use the Debugger hook If you are in a non-SageMaker environment, or even in SageMaker, if you want to configure the hook in a certain way in script mode, you can use the full Debugger hook API as follows. import xgboost as xgb from smdebug.xgboost import Hook dtrain = xgb.DMatrix(\"train.libsvm\") dvalid = xgb.DMatrix(\"validation.libsmv\") hook = Hook( out_dir=out_dir, # required train_data=dtrain, # required validation_data=dvalid, # optional hyperparameters=hyperparameters, # optional ) Full API def __init__( self, out_dir, export_tensorboard = False, tensorboard_dir = None, dry_run = False, reduction_config = None, save_config = None, include_regex = None, include_collections = None, save_all = False, include_workers = \"one\", hyperparameters = None, train_data = None, validation_data = None, ) Initializes the hook. Pass this object as a callback to xgboost.train() . * out_dir (str): A path into which tensors and metadata will be written. * export_tensorboard (bool): Whether to use TensorBoard logs. * tensorboard_dir (str): Where to save TensorBoard logs. * dry_run (bool): If true, evaluations are not actually saved to disk. * reduction_config (ReductionConfig object): Not supported in XGBoost and will be ignored. * save_config (SaveConfig object): See the Common API . * include_regex (list[str]): List of additional regexes to save. * include_collections (list[str]): List of collections to save. * save_all (bool): Saves all tensors and collections. WARNING: May be memory-intensive and slow. * include_workers (str): Used for distributed training, can also be \"all\". * hyperparameters (dict): Booster params. * train_data (DMatrix object): Data to be trained. * validation_data (DMatrix object): Validation set for which metrics will evaluated during training. See the Common API page for details about Collection, SaveConfig, and ReductionConfig.\\ See the Analysis page for details about analyzing a training job.","title":"XGBoost"},{"location":"xgboost/#xgboost","text":"","title":"XGBoost"},{"location":"xgboost/#contents","text":"SageMaker Example Full API","title":"Contents"},{"location":"xgboost/#sagemaker-example","text":"","title":"SageMaker Example"},{"location":"xgboost/#use-xgboost-as-a-built-in-algorithm","text":"The XGBoost algorithm can be used 1) as a built-in algorithm, or 2) as a framework such as MXNet, PyTorch, or Tensorflow. If SageMaker XGBoost is used as a built-in algorithm in container version 0.90-2 or later, Amazon SageMaker Debugger will be available by default (i.e., zero code change experience). See XGBoost Algorithm AWS docmentation for more information on how to use XGBoost as a built-in algorithm. See Amazon SageMaker Debugger examples for sample notebooks that demonstrate debugging and monitoring capabilities of Amazon SageMaker Debugger. See SageMaker Python SDK for more information on how to configure the Amazon SageMaker Debugger from the Python SDK.","title":"Use XGBoost as a built-in algorithm"},{"location":"xgboost/#use-xgboost-as-a-framework","text":"When SageMaker XGBoost is used as a framework, it is recommended that the hook is configured from the SageMaker Python SDK . By using SageMaker Python SDK, you can run different jobs (e.g., Processing jobs) on the SageMaker platform. You can retrieve the hook as follows. import xgboost as xgb from smdebug.xgboost import Hook dtrain = xgb.DMatrix(\"train.libsvm\") dtest = xgb.DMatrix(\"test.libsmv\") hook = Hook.create_from_json_file() hook.train_data = dtrain # required hook.validation_data = dtest # optional hook.hyperparameters = params # optional bst = xgb.train( params, dtrain, callbacks=[hook], evals_result=[(dtrain, \"train\"), (dvalid, \"validation\")] ) Alternatively, you can also create the hook from smdebug 's Python API as shown in the next section.","title":"Use XGBoost as a framework"},{"location":"xgboost/#use-the-debugger-hook","text":"If you are in a non-SageMaker environment, or even in SageMaker, if you want to configure the hook in a certain way in script mode, you can use the full Debugger hook API as follows. import xgboost as xgb from smdebug.xgboost import Hook dtrain = xgb.DMatrix(\"train.libsvm\") dvalid = xgb.DMatrix(\"validation.libsmv\") hook = Hook( out_dir=out_dir, # required train_data=dtrain, # required validation_data=dvalid, # optional hyperparameters=hyperparameters, # optional )","title":"Use the Debugger hook"},{"location":"xgboost/#full-api","text":"def __init__( self, out_dir, export_tensorboard = False, tensorboard_dir = None, dry_run = False, reduction_config = None, save_config = None, include_regex = None, include_collections = None, save_all = False, include_workers = \"one\", hyperparameters = None, train_data = None, validation_data = None, ) Initializes the hook. Pass this object as a callback to xgboost.train() . * out_dir (str): A path into which tensors and metadata will be written. * export_tensorboard (bool): Whether to use TensorBoard logs. * tensorboard_dir (str): Where to save TensorBoard logs. * dry_run (bool): If true, evaluations are not actually saved to disk. * reduction_config (ReductionConfig object): Not supported in XGBoost and will be ignored. * save_config (SaveConfig object): See the Common API . * include_regex (list[str]): List of additional regexes to save. * include_collections (list[str]): List of collections to save. * save_all (bool): Saves all tensors and collections. WARNING: May be memory-intensive and slow. * include_workers (str): Used for distributed training, can also be \"all\". * hyperparameters (dict): Booster params. * train_data (DMatrix object): Data to be trained. * validation_data (DMatrix object): Validation set for which metrics will evaluated during training. See the Common API page for details about Collection, SaveConfig, and ReductionConfig.\\ See the Analysis page for details about analyzing a training job.","title":"Full API"}]}